{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb64c020",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation for Random Forest\n",
    "\n",
    "This assignment is due on:\n",
    "\n",
    "- Tuesday 20th (February) at 11:59pm\n",
    "\n",
    "Send me your Jupyter Notebook to my email (eddiezago500@gmail.com) in the following format: hw2_name_surname.ipynb. Make sure to copy the whole 'assignment2' folder to your DropBox or local computer and use relative paths, as we have seen in class. I should be able to run all your codes without changing the paths.\n",
    "\n",
    "e.g: '../data/input/file.xlsx'\n",
    "\n",
    "Overall Instructions:\n",
    "\n",
    "In Machine Learning, while evaluating metrics from a certain model we usually want to make sure that this metrics are not dependent of the randomness induced at separating the data set into training and test. We want to make sure that our results are not dependent of a particular choice of training and validation sets.\n",
    "\n",
    "To do this we will use K-Fold Cross Validation, which consists of splitting our dataset 4 or 5 times into training and test, such that the test data is always different, and train and evaluate our model keeping constant our hyper-parameters. The final validation accuracy would be the mean of the 4 or 5 individual accuracies obtained.\n",
    "\n",
    "We will do this for the example seen in class using the Random Forest:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be249bd6",
   "metadata": {},
   "source": [
    "#### 1) Import the necessary packages to run a Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e64056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4b410a2",
   "metadata": {},
   "source": [
    "#### 1.1) import the Iris_reproducible data set and generate X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c801e01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd3be436",
   "metadata": {},
   "source": [
    "#### 2) Run the following code, which is the one from the homework with the optimal hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test using the same seed:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, random_state=41)\n",
    "\n",
    "# Define and train the ensemble with the optimal hyper-parameters:\n",
    "clf=RandomForestClassifier(n_estimators=10, max_depth = 5, \n",
    "                           min_samples_leaf = 1, min_samples_split = 3, \n",
    "                           random_state = 43)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "# Accuracy:\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35b2ed",
   "metadata": {},
   "source": [
    "#### 3) Run the code for 6 different seeds (at the train_test_split() function). Store the 6 accuracies in a list and print it\n",
    "\n",
    "(You can build a function, run a loop and store the accuracies or do it manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02cba4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d5dfb8e",
   "metadata": {},
   "source": [
    "If you've done this correctly, you will notice that the accuracy depends on the random division of the training and the test set. This usually happens with small training data sets as Iris, but it is also good practice to do Cross Validation when we have a big data set, for robustness check.\n",
    "\n",
    "To do it for the Random Forest, sklearn provides us with a function named cross_val_score.\n",
    "\n",
    "#### 4) Import the function cross_val_score from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed04b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0837e77",
   "metadata": {},
   "source": [
    "#### 4.1) Check the function documentation and calculate the cross-validation score (set cv = 5) for the clf model defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52590bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0466043",
   "metadata": {},
   "source": [
    "#### 4.2) Notice this will be a vector, obtain the final test accuracy by obtaining the mean of the 5 accuracies given by the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744997e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
