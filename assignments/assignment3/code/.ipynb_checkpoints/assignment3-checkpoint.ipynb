{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1859e579",
   "metadata": {},
   "source": [
    "## Credit Risk prediction using Neural Nets\n",
    "\n",
    "This assignment is due on:\n",
    "\n",
    "- \n",
    "\n",
    "Send me your Jupyter Notebook to my email (eddiezago500@gmail.com) in the following format: hw3_name_surname.ipynb. Make sure to copy the whole 'assignment3' folder to your DropBox or local computer and use relative paths, as we have seen in class. I should be able to run all your codes without changing the paths.\n",
    "\n",
    "e.g: '../data/input/file.xlsx'\n",
    "\n",
    "Overall Instructions:\n",
    "\n",
    "One interesting practical and business use of ML and DL is credit default prediction. Most big financial companies have rich data sets containing information of their clients credit history, personal information and if they defaulted or not when they received a loan. These datasets are frequently utilized to train models that subsequently aid companies in determining whether to extend credit to new clients, what interest rates to offer, and the appropriate loan amounts.\n",
    "\n",
    "The data set was obtained from Kaggle: https://www.kaggle.com/datasets/laotse/credit-risk-dataset/data. Follow that link to obtain the description for those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfabe586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333b700",
   "metadata": {},
   "source": [
    "1) Import the data set and take a look at all the variables using the function .info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b818380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b03ab30",
   "metadata": {},
   "source": [
    "2) Pre-processing: One important aspect of ML is data pre-processing. For example, ff you look closely at the data, you will notice that you have 3 categorical variables, that are coded as strings, and cannot be inputed into a model like that. The way to go with these variables is to generate dummies for each category-variable.\n",
    "\n",
    "2.1) Generate dummies for each categorical variable (person_home_ownership, loan_intent, loan_grade, cb_person_default_on_file). (Hint: you should end up with 29 columns and there is pandas function that does this for you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd73b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68482407",
   "metadata": {},
   "source": [
    "2.2) Remove any NA values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb42c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a699c977",
   "metadata": {},
   "source": [
    "2.3) Plot the distribution of labels using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59020b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6472857",
   "metadata": {},
   "source": [
    "As per usual with credit risk datasets, there is a clear over sampling in the no default class. There are usually many more people that do pay up their loans than people that default (if not all banks would go bankrupt). \n",
    "\n",
    "We can solve it using the following function, which I will provide to you (you just have to run it)\n",
    "\n",
    "2.4) Define X (set of predictors) and y (label) then run the code provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825dc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0b24ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ebcb1",
   "metadata": {},
   "source": [
    "2.5) It is also, great practice, to remove any outliers that might affect the prediction of our model. For this, we can drop them or standarize them so that they are comparable between each other (with respect to the standard deviation) and have less variance. The function is also provided to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de92bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_resampled)\n",
    "x_scaled = scaler.transform(x_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac29526",
   "metadata": {},
   "source": [
    "3) Now we have the data base ready to train our model and do the prediction. Let us start with a benchmark model, such as the random forest. \n",
    "\n",
    "3.1) Split the dataset into training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5287a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d1c2c52",
   "metadata": {},
   "source": [
    "3.2) Train a random forest (without using the hyper-parameter optimal grid, since it takes too much time, so use the initial one in the notes) and print and record the training and validation accuracy. Comment on the results (do you have over-fitting? what can you do to improve the accuracy?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedabad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d443e620",
   "metadata": {},
   "source": [
    "4) It is common practice, while training a model for prediction, to try different models, different sets of hyper-parameters, and to choose the best one. Given this, in this last section we are going to try surpass the prediction of the random forest using Neural Nets.\n",
    "\n",
    "4.1) Define (with code) and train a (Neural Net with more than one node and more than 1 hidden layer). Report the validation accuracy.\n",
    "\n",
    "pd. You have freedom to choose all the other relevant hyper-parameters, I just set up for you the AdamW optimizer (which is the most commonly used) and the loss and metrics. Recall the initial shape is equal to the number of X variables.\n",
    "\n",
    "Extra point if you surpass the 90% accuracy threshold. Leave the print in the code, so I do not have to spend time re-training each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcbcc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1559093",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=.001)\n",
    "\n",
    "MLP.compile(optimizer=optimizer, loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180ff6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915e9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cd3ac03",
   "metadata": {},
   "source": [
    "4.2) Finally, plot the Learning Curve (only training and validation accuracy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63acedfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
