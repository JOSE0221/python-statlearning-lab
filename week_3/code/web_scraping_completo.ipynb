{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåê Web Scraping con Python: Curso Completo y Riguroso\n",
    "\n",
    "## Notebook Integrado con Todas las T√©cnicas State-of-the-Art\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Contenido del Curso\n",
    "\n",
    "| Parte | Tema | Duraci√≥n |\n",
    "|-------|------|----------|\n",
    "| **1** | Fundamentos HTTP y Requests | ~45 min |\n",
    "| **2** | T√©cnicas de Extracci√≥n (Regex, BeautifulSoup, lxml) | ~50 min |\n",
    "| **3** | Crawlers (BFS, DFS, Concurrente) | ~50 min |\n",
    "| **4** | T√©cnicas Anti-Ban y Evasi√≥n de Detecci√≥n | ~50 min |\n",
    "| **5** | Selenium y Automatizaci√≥n de Navegador | ~50 min |\n",
    "| **6** | Proyecto Final Integrado | ~60 min |\n",
    "| **7** | An√°lisis de Datos Scrapeados (CSV Integration) | ~30 min |\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ Referencia Principal\n",
    "- Jarmul, K. & Lawson, R. (2017). *Python Web Scraping*, 2nd Edition. Packt Publishing.\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Curso preparado para el Seminario de Investigaci√≥n Aplicada  \n",
    "**Fecha:** Enero 2025  \n",
    "**Versi√≥n:** 2.0 - Completa e Integrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üì¶ INSTALACI√ìN Y CONFIGURACI√ìN INICIAL\n",
    "\n",
    "Ejecuta esta celda primero para instalar todas las dependencias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.13/site-packages (2.32.5)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting lxml\n",
      "  Using cached lxml-6.0.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Collecting cssselect\n",
      "  Using cached cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-3.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (79 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.4.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.13/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.13/site-packages (from requests) (2026.1.4)\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.8.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/miniconda3/lib/python3.13/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: six>=1.9 in /opt/miniconda3/lib/python3.13/site-packages (from html5lib) (1.17.0)\n",
      "Collecting webencodings (from html5lib)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting anyio (from httpx)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting httpcore==1.* (from httpx)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Using cached lxml-6.0.2-cp313-cp313-macosx_10_13_universal2.whl (8.6 MB)\n",
      "Using cached cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading pandas-3.0.0-cp313-cp313-macosx_11_0_arm64.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.1-cp313-cp313-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading soupsieve-2.8.3-py3-none-any.whl (37 kB)\n",
      "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, soupsieve, numpy, lxml, html5lib, h11, cssselect, anyio, pandas, httpcore, beautifulsoup4, httpx\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12/12\u001b[0m [httpx]m11/12\u001b[0m [httpx]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed anyio-4.12.1 beautifulsoup4-4.14.3 cssselect-1.3.0 h11-0.16.0 html5lib-1.1 httpcore-1.0.9 httpx-0.28.1 lxml-6.0.2 numpy-2.4.1 pandas-3.0.0 soupsieve-2.8.3 webencodings-0.5.1\n",
      "Collecting fake-useragent\n",
      "  Using cached fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.40.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: certifi>=2026.1.4 in /opt/miniconda3/lib/python3.13/site-packages (from selenium) (2026.1.4)\n",
      "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
      "  Using cached trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting trio-typing>=0.10.0 (from selenium)\n",
      "  Downloading trio_typing-0.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting types-certifi>=2021.10.8.3 (from selenium)\n",
      "  Downloading types_certifi-2021.10.8.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting types-urllib3>=1.26.25.14 (from selenium)\n",
      "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /opt/miniconda3/lib/python3.13/site-packages (from selenium) (4.15.0)\n",
      "Collecting urllib3<3.0,>=2.6.3 (from urllib3[socks]<3.0,>=2.6.3->selenium)\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting websocket-client<2.0,>=1.8.0 (from selenium)\n",
      "  Downloading websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting attrs>=23.2.0 (from trio<1.0,>=0.31.0->selenium)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /opt/miniconda3/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
      "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Downloading wsproto-1.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/miniconda3/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.6.3->selenium) (1.7.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.13/site-packages (from webdriver-manager) (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in /opt/miniconda3/lib/python3.13/site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.13/site-packages (from webdriver-manager) (25.0)\n",
      "Collecting mypy-extensions>=0.4.2 (from trio-typing>=0.10.0->selenium)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting async-generator (from trio-typing>=0.10.0->selenium)\n",
      "  Downloading async_generator-1.10-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting importlib-metadata (from trio-typing>=0.10.0->selenium)\n",
      "  Downloading importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in /opt/miniconda3/lib/python3.13/site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Collecting zipp>=3.20 (from importlib-metadata->trio-typing>=0.10.0->selenium)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.13/site-packages (from requests->webdriver-manager) (3.4.4)\n",
      "Using cached fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
      "Downloading selenium-4.40.0-py3-none-any.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached trio-0.32.0-py3-none-any.whl (512 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Downloading websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading trio_typing-0.10.0-py3-none-any.whl (42 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading types_certifi-2021.10.8.3-py3-none-any.whl (2.1 kB)\n",
      "Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
      "Downloading wsproto-1.3.2-py3-none-any.whl (24 kB)\n",
      "Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Downloading importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: types-urllib3, types-certifi, sortedcontainers, zipp, wsproto, websocket-client, urllib3, sniffio, mypy-extensions, fake-useragent, attrs, async-generator, outcome, importlib-metadata, webdriver-manager, trio, trio-websocket, trio-typing, selenium\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19/19\u001b[0m [selenium]/19\u001b[0m [selenium]\n",
      "\u001b[1A\u001b[2KSuccessfully installed async-generator-1.10 attrs-25.4.0 fake-useragent-2.2.0 importlib-metadata-8.7.1 mypy-extensions-1.1.0 outcome-1.3.0.post0 selenium-4.40.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.32.0 trio-typing-0.10.0 trio-websocket-0.12.2 types-certifi-2021.10.8.3 types-urllib3-1.26.25.14 urllib3-2.6.3 webdriver-manager-4.0.2 websocket-client-1.9.0 wsproto-1.3.2 zipp-3.23.0\n",
      "\n",
      "‚úÖ Instalaci√≥n completada!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# INSTALACI√ìN DE PAQUETES\n",
    "# ==============================================================================\n",
    "# Ejecutar solo una vez para instalar todas las dependencias\n",
    "\n",
    "!pip install requests beautifulsoup4 lxml cssselect html5lib pandas numpy tqdm httpx\n",
    "!pip install fake-useragent selenium webdriver-manager\n",
    "\n",
    "# Opcional: para anti-detecci√≥n avanzada\n",
    "# !pip install undetected-chromedriver playwright\n",
    "\n",
    "print(\"\\n‚úÖ Instalaci√≥n completada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ fake_useragent disponible\n",
      "‚úÖ Selenium disponible\n",
      "\n",
      "============================================================\n",
      "‚úÖ TODOS LOS IMPORTS COMPLETADOS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS MAESTROS - TODAS LAS BIBLIOTECAS DEL CURSO\n",
    "# ==============================================================================\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Bibliotecas est√°ndar de Python\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import re                    # Expresiones regulares para extracci√≥n de patrones\n",
    "import os                    # Operaciones del sistema de archivos\n",
    "import sys                   # Informaci√≥n del sistema\n",
    "import csv                   # Lectura/escritura de archivos CSV\n",
    "import json                  # Serializaci√≥n JSON\n",
    "import time                  # Control de tiempo y delays\n",
    "import random                # Generaci√≥n de n√∫meros aleatorios\n",
    "import hashlib               # Funciones hash para deduplicaci√≥n\n",
    "import logging               # Sistema de logging\n",
    "import sqlite3               # Base de datos SQLite\n",
    "from typing import (         # Type hints para mejor documentaci√≥n\n",
    "    List, Dict, Set, Optional, Any, Tuple, \n",
    "    Callable, Generator, Union, Pattern\n",
    ")\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin, urlparse, urlunparse, parse_qs, urlencode\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from collections import deque, defaultdict, Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Bibliotecas de terceros para HTTP\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import requests                              # Cliente HTTP principal\n",
    "from requests.adapters import HTTPAdapter    # Adaptador para configuraci√≥n\n",
    "from urllib3.util.retry import Retry         # Estrategia de reintentos\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Bibliotecas para parsing HTML\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from bs4 import BeautifulSoup                # Parser HTML amigable\n",
    "from lxml import html as lxml_html           # Parser HTML r√°pido\n",
    "from lxml import etree                       # Soporte XPath avanzado\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Bibliotecas para an√°lisis de datos\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import pandas as pd                          # DataFrames y an√°lisis\n",
    "import numpy as np                           # Operaciones num√©ricas\n",
    "from tqdm.auto import tqdm                   # Barras de progreso\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Configuraci√≥n de logging\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger('WebScraping')\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Imports opcionales (pueden no estar instalados)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    from fake_useragent import UserAgent\n",
    "    FAKE_UA_AVAILABLE = True\n",
    "    print(\"‚úÖ fake_useragent disponible\")\n",
    "except ImportError:\n",
    "    FAKE_UA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è fake_useragent no disponible\")\n",
    "\n",
    "try:\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    SELENIUM_AVAILABLE = True\n",
    "    print(\"‚úÖ Selenium disponible\")\n",
    "except ImportError:\n",
    "    SELENIUM_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Selenium no disponible\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TODOS LOS IMPORTS COMPLETADOS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PARTE 1: FUNDAMENTOS HTTP Y REQUESTS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "## 1.1 El Protocolo HTTP: La Base del Web Scraping\n",
    "\n",
    "El protocolo HTTP (HyperText Transfer Protocol) es el fundamento de toda comunicaci√≥n web.\n",
    "Entenderlo es **ESENCIAL** para hacer web scraping.\n",
    "\n",
    "### Anatom√≠a de una Petici√≥n HTTP\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    PETICI√ìN HTTP                            ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ GET /productos?categoria=electronica HTTP/1.1               ‚îÇ  ‚Üê L√≠nea de solicitud\n",
    "‚îÇ Host: www.ejemplo.com                                       ‚îÇ  ‚Üê Header obligatorio\n",
    "‚îÇ User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64)...    ‚îÇ  ‚Üê Identifica el cliente\n",
    "‚îÇ Accept: text/html,application/xhtml+xml                     ‚îÇ  ‚Üê Tipos aceptados\n",
    "‚îÇ Accept-Language: es-MX,es;q=0.9,en;q=0.8                    ‚îÇ  ‚Üê Idiomas preferidos\n",
    "‚îÇ Accept-Encoding: gzip, deflate, br                          ‚îÇ  ‚Üê Compresiones aceptadas\n",
    "‚îÇ Connection: keep-alive                                      ‚îÇ  ‚Üê Mantener conexi√≥n\n",
    "‚îÇ Cookie: session_id=abc123; user_pref=dark                   ‚îÇ  ‚Üê Cookies del navegador\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ [Cuerpo de la petici√≥n - vac√≠o para GET]                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EJEMPLO: PETICI√ìN HTTP COMPLETA\n",
      "======================================================================\n",
      "\n",
      "URL: https://httpbin.org/get\n",
      "\n",
      "Headers enviados:\n",
      "  User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...\n",
      "  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/...\n",
      "  Accept-Language: es-MX,es;q=0.9,en;q=0.8\n",
      "  Accept-Encoding: gzip, deflate, br\n",
      "  Connection: keep-alive\n",
      "  DNT: 1\n",
      "  Upgrade-Insecure-Requests: 1\n",
      "  Cache-Control: max-age=0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RESPUESTA RECIBIDA:\n",
      "----------------------------------------------------------------------\n",
      "Status Code: 200\n",
      "Content-Type: application/json\n",
      "Encoding: utf-8\n",
      "Tiempo de respuesta: 0.560 segundos\n",
      "\n",
      "Headers que el servidor vio:\n",
      "  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/...\n",
      "  Accept-Encoding: gzip, deflate, br\n",
      "  Accept-Language: es-MX,es;q=0.9,en;q=0.8\n",
      "  Cache-Control: max-age=0\n",
      "  Dnt: 1\n",
      "  Host: httpbin.org\n",
      "  Upgrade-Insecure-Requests: 1\n",
      "  User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...\n",
      "  X-Amzn-Trace-Id: Root=1-6979f7f9-063974091947ffe612a4783b\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1.1 ANATOM√çA DE UNA PETICI√ìN HTTP COMPLETA\n",
    "# ==============================================================================\n",
    "\n",
    "def ejemplo_peticion_http_completa():\n",
    "    \"\"\"\n",
    "    Demuestra una petici√≥n HTTP completa con todos los headers.\n",
    "    \n",
    "    Esta funci√≥n muestra c√≥mo un navegador real env√≠a peticiones,\n",
    "    incluyendo todos los headers que los sitios web esperan ver.\n",
    "    \"\"\"\n",
    "    \n",
    "    # URL de prueba - httpbin.org es un servicio que devuelve informaci√≥n\n",
    "    # sobre la petici√≥n que recibe, perfecto para aprender\n",
    "    url = \"https://httpbin.org/get\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    # HEADERS: La \"identidad\" de nuestra petici√≥n\n",
    "    # =========================================================================\n",
    "    # Los headers son metadatos que acompa√±an cada petici√≥n.\n",
    "    # Son CRUCIALES porque los sitios los usan para detectar bots.\n",
    "    \n",
    "    headers = {\n",
    "        # User-Agent: Identifica el navegador/cliente\n",
    "        # Este es el header M√ÅS IMPORTANTE para evitar detecci√≥n\n",
    "        # Un User-Agent como \"python-requests/2.28.0\" grita \"SOY UN BOT\"\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        \n",
    "        # Accept: Tipos MIME que el cliente acepta\n",
    "        # text/html = p√°ginas web, application/json = APIs\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "        \n",
    "        # Accept-Language: Idiomas preferidos\n",
    "        # q=0.9 significa \"90% de preferencia\"\n",
    "        'Accept-Language': 'es-MX,es;q=0.9,en;q=0.8',\n",
    "        \n",
    "        # Accept-Encoding: Compresiones que el cliente puede descomprimir\n",
    "        # gzip y br (Brotli) son los m√°s comunes\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        \n",
    "        # Connection: C√≥mo manejar la conexi√≥n TCP\n",
    "        # keep-alive = reutilizar conexi√≥n para m√∫ltiples peticiones\n",
    "        'Connection': 'keep-alive',\n",
    "        \n",
    "        # DNT (Do Not Track): Preferencia de privacidad\n",
    "        # 1 = no rastrear (aunque los sitios pueden ignorarlo)\n",
    "        'DNT': '1',\n",
    "        \n",
    "        # Upgrade-Insecure-Requests: Preferir HTTPS sobre HTTP\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        \n",
    "        # Cache-Control: Instrucciones de cach√©\n",
    "        'Cache-Control': 'max-age=0',\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"EJEMPLO: PETICI√ìN HTTP COMPLETA\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nURL: {url}\")\n",
    "    print(f\"\\nHeaders enviados:\")\n",
    "    for key, value in headers.items():\n",
    "        print(f\"  {key}: {value[:60]}{'...' if len(value) > 60 else ''}\")\n",
    "    \n",
    "    # Realizar la petici√≥n\n",
    "    response = requests.get(url, headers=headers, timeout=30)\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(\"RESPUESTA RECIBIDA:\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"Content-Type: {response.headers.get('Content-Type')}\")\n",
    "    print(f\"Encoding: {response.encoding}\")\n",
    "    print(f\"Tiempo de respuesta: {response.elapsed.total_seconds():.3f} segundos\")\n",
    "    \n",
    "    # httpbin.org devuelve JSON con los detalles de nuestra petici√≥n\n",
    "    data = response.json()\n",
    "    print(f\"\\nHeaders que el servidor vio:\")\n",
    "    for key, value in data.get('headers', {}).items():\n",
    "        print(f\"  {key}: {value[:60]}{'...' if len(str(value)) > 60 else ''}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Ejecutar ejemplo\n",
    "response = ejemplo_peticion_http_completa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 C√≥digos de Estado HTTP\n",
    "\n",
    "Los c√≥digos de estado nos dicen qu√© pas√≥ con nuestra petici√≥n:\n",
    "\n",
    "| C√≥digo | Significado | Acci√≥n en Scraping |\n",
    "|--------|-------------|--------------------|\n",
    "| **200** | OK - √âxito | ‚úÖ Procesar contenido |\n",
    "| **301/302** | Redirecci√≥n | Seguir nueva URL (requests lo hace autom√°tico) |\n",
    "| **400** | Bad Request | Revisar par√°metros/headers |\n",
    "| **401** | No autorizado | Necesita autenticaci√≥n |\n",
    "| **403** | Prohibido | Posible detecci√≥n de bot ‚ö†Ô∏è |\n",
    "| **404** | No encontrado | URL inv√°lida, skip |\n",
    "| **429** | Too Many Requests | Rate limited - reducir velocidad ‚ö†Ô∏è |\n",
    "| **500** | Error del servidor | Reintentar m√°s tarde |\n",
    "| **503** | Servicio no disponible | Reintentar m√°s tarde |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMOSTRACI√ìN DE C√ìDIGOS DE ESTADO HTTP\n",
      "======================================================================\n",
      "\n",
      "‚úÖ 200: OK - La petici√≥n fue exitosa\n",
      "   Respuesta real: 200\n",
      "\n",
      "‚Ü™Ô∏è 301: Moved Permanently - Recurso movido\n",
      "   Respuesta real: 301\n",
      "\n",
      "‚ö†Ô∏è 400: Bad Request - Petici√≥n malformada\n",
      "   Respuesta real: 400\n",
      "\n",
      "‚ö†Ô∏è 403: Forbidden - Acceso prohibido (¬øBot detectado?)\n",
      "   Respuesta real: 403\n",
      "\n",
      "‚ö†Ô∏è 404: Not Found - Recurso no existe\n",
      "   Respuesta real: 404\n",
      "\n",
      "‚ö†Ô∏è 429: Too Many Requests - Rate limited\n",
      "   Respuesta real: 429\n",
      "\n",
      "‚ùå 500: Internal Server Error - Error del servidor\n",
      "   Respuesta real: 500\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1.2 DEMOSTRACI√ìN DE C√ìDIGOS DE ESTADO HTTP\n",
    "# ==============================================================================\n",
    "\n",
    "def demostrar_codigos_http():\n",
    "    \"\"\"\n",
    "    Demuestra diferentes c√≥digos de estado HTTP.\n",
    "    httpbin.org permite simular cualquier c√≥digo de respuesta.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Diccionario de c√≥digos a probar con sus significados\n",
    "    codigos = {\n",
    "        200: \"OK - La petici√≥n fue exitosa\",\n",
    "        301: \"Moved Permanently - Recurso movido\",\n",
    "        400: \"Bad Request - Petici√≥n malformada\",\n",
    "        403: \"Forbidden - Acceso prohibido (¬øBot detectado?)\",\n",
    "        404: \"Not Found - Recurso no existe\",\n",
    "        429: \"Too Many Requests - Rate limited\",\n",
    "        500: \"Internal Server Error - Error del servidor\",\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"DEMOSTRACI√ìN DE C√ìDIGOS DE ESTADO HTTP\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for codigo, significado in codigos.items():\n",
    "        try:\n",
    "            # httpbin.org/status/{code} devuelve ese c√≥digo\n",
    "            url = f\"https://httpbin.org/status/{codigo}\"\n",
    "            \n",
    "            # allow_redirects=False para ver el 301 real\n",
    "            response = requests.get(url, timeout=10, allow_redirects=False)\n",
    "            \n",
    "            # Emoji seg√∫n tipo de respuesta\n",
    "            if 200 <= response.status_code < 300:\n",
    "                emoji = \"‚úÖ\"\n",
    "            elif 300 <= response.status_code < 400:\n",
    "                emoji = \"‚Ü™Ô∏è\"\n",
    "            elif 400 <= response.status_code < 500:\n",
    "                emoji = \"‚ö†Ô∏è\"\n",
    "            else:\n",
    "                emoji = \"‚ùå\"\n",
    "            \n",
    "            print(f\"\\n{emoji} {codigo}: {significado}\")\n",
    "            print(f\"   Respuesta real: {response.status_code}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùì {codigo}: Error - {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "demostrar_codigos_http()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Sesiones y Cookies\n",
    "\n",
    "HTTP es **stateless** (sin estado). Las sesiones y cookies permiten mantener estado entre peticiones.\n",
    "\n",
    "### ¬øPor qu√© usar `requests.Session()`?\n",
    "\n",
    "1. **Persistencia de cookies**: Las cookies se mantienen autom√°ticamente\n",
    "2. **Reutilizaci√≥n de conexiones**: M√°s r√°pido (no reconecta cada vez)\n",
    "3. **Headers compartidos**: Configurar una vez, usar siempre\n",
    "4. **Autenticaci√≥n**: Login persiste entre peticiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMOSTRACI√ìN DE SESIONES Y COOKIES\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ Sesi√≥n creada con headers por defecto\n",
      "\n",
      "2Ô∏è‚É£ Primera petici√≥n: https://httpbin.org/cookies/set/session_id/abc123\n",
      "   Status: 502\n",
      "   Cookies en la sesi√≥n: {'session_id': 'abc123'}\n",
      "\n",
      "3Ô∏è‚É£ Segunda petici√≥n (cookies enviadas autom√°ticamente):\n",
      "   El servidor vio estas cookies: {'session_id': 'abc123'}\n",
      "\n",
      "4Ô∏è‚É£ Comparaci√≥n: Petici√≥n SIN sesi√≥n\n",
      "   El servidor vio: {}\n",
      "   (vac√≠o porque no enviamos cookies)\n",
      "\n",
      "======================================================================\n",
      "CONCLUSI√ìN: Siempre usa Session() para scraping\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1.3 SESIONES Y MANEJO DE COOKIES\n",
    "# ==============================================================================\n",
    "\n",
    "def demostrar_sesiones_cookies():\n",
    "    \"\"\"\n",
    "    Demuestra el uso de sesiones y c√≥mo manejar cookies.\n",
    "    \n",
    "    Las sesiones son FUNDAMENTALES para:\n",
    "    - Mantener login entre peticiones\n",
    "    - Parecer un usuario real (mismas cookies)\n",
    "    - Reutilizar conexiones (m√°s r√°pido)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"DEMOSTRACI√ìN DE SESIONES Y COOKIES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CREAR SESI√ìN\n",
    "    # =========================================================================\n",
    "    # Una sesi√≥n es como abrir un navegador: mantiene estado\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Configurar headers por defecto para toda la sesi√≥n\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'es-MX,es;q=0.9',\n",
    "    })\n",
    "    \n",
    "    print(\"\\n1Ô∏è‚É£ Sesi√≥n creada con headers por defecto\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PRIMERA PETICI√ìN: El servidor nos da cookies\n",
    "    # =========================================================================\n",
    "    # httpbin.org/cookies/set establece cookies\n",
    "    url_set = \"https://httpbin.org/cookies/set/session_id/abc123\"\n",
    "    response1 = session.get(url_set, allow_redirects=True)\n",
    "    \n",
    "    print(f\"\\n2Ô∏è‚É£ Primera petici√≥n: {url_set}\")\n",
    "    print(f\"   Status: {response1.status_code}\")\n",
    "    print(f\"   Cookies en la sesi√≥n: {dict(session.cookies)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SEGUNDA PETICI√ìN: Las cookies se env√≠an autom√°ticamente\n",
    "    # =========================================================================\n",
    "    url_get = \"https://httpbin.org/cookies\"\n",
    "    response2 = session.get(url_get)\n",
    "    \n",
    "    print(f\"\\n3Ô∏è‚É£ Segunda petici√≥n (cookies enviadas autom√°ticamente):\")\n",
    "    print(f\"   El servidor vio estas cookies: {response2.json().get('cookies')}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # COMPARACI√ìN: Sin sesi√≥n las cookies NO se mantienen\n",
    "    # =========================================================================\n",
    "    print(f\"\\n4Ô∏è‚É£ Comparaci√≥n: Petici√≥n SIN sesi√≥n\")\n",
    "    response_sin = requests.get(url_get)\n",
    "    print(f\"   El servidor vio: {response_sin.json().get('cookies')}\")\n",
    "    print(f\"   (vac√≠o porque no enviamos cookies)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONCLUSI√ìN: Siempre usa Session() para scraping\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return session\n",
    "\n",
    "session = demostrar_sesiones_cookies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 robots.txt: √âtica y Legalidad\n",
    "\n",
    "El archivo `robots.txt` indica qu√© puede y qu√© no puede crawlear un bot.\n",
    "\n",
    "```\n",
    "# Ejemplo de robots.txt\n",
    "User-agent: *              # Aplica a todos los bots\n",
    "Disallow: /admin/          # No acceder a /admin/\n",
    "Disallow: /private/        # No acceder a /private/\n",
    "Allow: /public/            # S√≠ se puede acceder a /public/\n",
    "Crawl-delay: 10            # Esperar 10 segundos entre peticiones\n",
    "Sitemap: /sitemap.xml      # Ubicaci√≥n del sitemap\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Importante\n",
    "- `robots.txt` es una **convenci√≥n**, no una restricci√≥n t√©cnica\n",
    "- Respetarlo es **√©tico y profesional**\n",
    "- Ignorarlo puede tener **consecuencias legales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMOSTRACI√ìN DE ROBOTS.TXT\n",
      "======================================================================\n",
      "\n",
      "üìã Verificando: https://www.wikipedia.org/wiki/Python\n",
      "‚úÖ robots.txt cargado: https://www.wikipedia.org/robots.txt\n",
      "   ‚ùå PROHIBIDO\n",
      "\n",
      "üìã Verificando: https://www.google.com/search?q=test\n",
      "‚úÖ robots.txt cargado: https://www.google.com/robots.txt\n",
      "   ‚ùå PROHIBIDO\n",
      "\n",
      "üìã Verificando: https://twitter.com/home\n",
      "‚úÖ robots.txt cargado: https://twitter.com/robots.txt\n",
      "   ‚ùå PROHIBIDO\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1.4 ROBOTS.TXT - VERIFICADOR DE PERMISOS\n",
    "# ==============================================================================\n",
    "\n",
    "class RobotsChecker:\n",
    "    \"\"\"\n",
    "    Verificador de robots.txt para crawling √©tico.\n",
    "    \n",
    "    Esta clase parsea y verifica robots.txt de cualquier sitio\n",
    "    para determinar si podemos acceder a una URL.\n",
    "    \n",
    "    Attributes:\n",
    "        parsers: Cache de RobotFileParser por dominio\n",
    "    \n",
    "    Example:\n",
    "        >>> checker = RobotsChecker()\n",
    "        >>> if checker.puede_acceder(\"https://wikipedia.org/wiki/Python\"):\n",
    "        ...     # OK para scrapear\n",
    "        ...     pass\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, user_agent: str = \"*\"):\n",
    "        \"\"\"\n",
    "        Inicializa el verificador.\n",
    "        \n",
    "        Args:\n",
    "            user_agent: User-Agent para verificar permisos.\n",
    "                        \"*\" verifica las reglas generales.\n",
    "        \"\"\"\n",
    "        self.user_agent = user_agent\n",
    "        self.parsers: Dict[str, RobotFileParser] = {}\n",
    "    \n",
    "    def _get_parser(self, url: str) -> Optional[RobotFileParser]:\n",
    "        \"\"\"\n",
    "        Obtiene o crea un parser de robots.txt para el dominio.\n",
    "        \n",
    "        Args:\n",
    "            url: URL del sitio\n",
    "            \n",
    "        Returns:\n",
    "            RobotFileParser o None si no se puede acceder\n",
    "        \"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        domain = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "        \n",
    "        if domain not in self.parsers:\n",
    "            robots_url = f\"{domain}/robots.txt\"\n",
    "            parser = RobotFileParser()\n",
    "            parser.set_url(robots_url)\n",
    "            \n",
    "            try:\n",
    "                parser.read()\n",
    "                self.parsers[domain] = parser\n",
    "                print(f\"‚úÖ robots.txt cargado: {robots_url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se pudo cargar robots.txt de {domain}: {e}\")\n",
    "                self.parsers[domain] = None\n",
    "        \n",
    "        return self.parsers[domain]\n",
    "    \n",
    "    def puede_acceder(self, url: str) -> bool:\n",
    "        \"\"\"\n",
    "        Verifica si podemos acceder a una URL seg√∫n robots.txt.\n",
    "        \n",
    "        Args:\n",
    "            url: URL a verificar\n",
    "            \n",
    "        Returns:\n",
    "            True si est√° permitido, False si est√° prohibido\n",
    "        \"\"\"\n",
    "        parser = self._get_parser(url)\n",
    "        \n",
    "        if parser is None:\n",
    "            return True  # Si no hay robots.txt, asumimos permitido\n",
    "        \n",
    "        return parser.can_fetch(self.user_agent, url)\n",
    "    \n",
    "    def obtener_crawl_delay(self, url: str) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Obtiene el Crawl-delay especificado en robots.txt.\n",
    "        \n",
    "        Args:\n",
    "            url: URL del sitio\n",
    "            \n",
    "        Returns:\n",
    "            Segundos de delay o None si no especificado\n",
    "        \"\"\"\n",
    "        parser = self._get_parser(url)\n",
    "        \n",
    "        if parser is None:\n",
    "            return None\n",
    "        \n",
    "        return parser.crawl_delay(self.user_agent)\n",
    "\n",
    "\n",
    "# Demostraci√≥n\n",
    "def demostrar_robots_txt():\n",
    "    \"\"\"Demuestra el uso del verificador de robots.txt.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"DEMOSTRACI√ìN DE ROBOTS.TXT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    checker = RobotsChecker()\n",
    "    \n",
    "    # URLs de prueba\n",
    "    urls_prueba = [\n",
    "        \"https://www.wikipedia.org/wiki/Python\",\n",
    "        \"https://www.google.com/search?q=test\",\n",
    "        \"https://twitter.com/home\",\n",
    "    ]\n",
    "    \n",
    "    for url in urls_prueba:\n",
    "        print(f\"\\nüìã Verificando: {url}\")\n",
    "        \n",
    "        permitido = checker.puede_acceder(url)\n",
    "        delay = checker.obtener_crawl_delay(url)\n",
    "        \n",
    "        status = \"‚úÖ PERMITIDO\" if permitido else \"‚ùå PROHIBIDO\"\n",
    "        print(f\"   {status}\")\n",
    "        \n",
    "        if delay:\n",
    "            print(f\"   ‚è±Ô∏è Crawl-delay: {delay} segundos\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "demostrar_robots_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una clase de descarga profesional que incluye:\n",
    "- Reintentos autom√°ticos con backoff exponencial\n",
    "- Throttling por dominio\n",
    "- Cach√© de respuestas\n",
    "- Rotaci√≥n de User-Agents\n",
    "- Logging detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMOSTRACI√ìN DEL DOWNLOADER ROBUSTO\n",
      "======================================================================\n",
      "\n",
      "üì• Descargando: https://httpbin.org/html\n",
      "‚úÖ robots.txt cargado: https://httpbin.org/robots.txt\n",
      "   ‚úÖ Status: 200\n",
      "   üì¶ Tama√±o: 3,739 bytes\n",
      "   ‚è±Ô∏è Tiempo: 0.476s\n",
      "   üíæ Desde cach√©: False\n",
      "\n",
      "üì• Descargando: https://httpbin.org/status/404\n",
      "   ‚ùå Error: Status 404\n",
      "\n",
      "üì• Descargando: https://httpbin.org/html\n",
      "   ‚úÖ Status: 200\n",
      "   üì¶ Tama√±o: 3,739 bytes\n",
      "   ‚è±Ô∏è Tiempo: 0.476s\n",
      "   üíæ Desde cach√©: True\n",
      "\n",
      "üìä Estad√≠sticas:\n",
      "   requests: 2\n",
      "   successes: 1\n",
      "   failures: 1\n",
      "   cache_hits: 1\n",
      "   success_rate: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1.5 CLASE DOWNLOADER ROBUSTA Y PROFESIONAL\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DownloadResult:\n",
    "    \"\"\"\n",
    "    Resultado estructurado de una descarga.\n",
    "    \n",
    "    Attributes:\n",
    "        url: URL descargada\n",
    "        status_code: C√≥digo HTTP de respuesta\n",
    "        content: Contenido descargado\n",
    "        headers: Headers de respuesta\n",
    "        elapsed: Tiempo de descarga en segundos\n",
    "        from_cache: Si vino del cach√©\n",
    "        error: Mensaje de error si fall√≥\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    status_code: Optional[int] = None\n",
    "    content: Optional[str] = None\n",
    "    headers: Dict[str, str] = field(default_factory=dict)\n",
    "    elapsed: float = 0.0\n",
    "    from_cache: bool = False\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def success(self) -> bool:\n",
    "        \"\"\"True si la descarga fue exitosa (2xx).\"\"\"\n",
    "        return self.status_code is not None and 200 <= self.status_code < 300\n",
    "\n",
    "\n",
    "class Downloader:\n",
    "    \"\"\"\n",
    "    Descargador robusto para web scraping profesional.\n",
    "    \n",
    "    Caracter√≠sticas:\n",
    "    - Reintentos autom√°ticos con backoff exponencial\n",
    "    - Throttling por dominio para no sobrecargar servidores\n",
    "    - Cach√© de respuestas para evitar descargas repetidas\n",
    "    - Rotaci√≥n de User-Agents\n",
    "    - Verificaci√≥n de robots.txt\n",
    "    \n",
    "    Example:\n",
    "        >>> downloader = Downloader(delay=2.0, max_retries=3)\n",
    "        >>> result = downloader.download(\"https://ejemplo.com\")\n",
    "        >>> if result.success:\n",
    "        ...     print(f\"Descargado: {len(result.content)} bytes\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # User-Agents realistas para rotaci√≥n\n",
    "    USER_AGENTS = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0\",\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        delay: float = 1.0,\n",
    "        max_retries: int = 3,\n",
    "        timeout: int = 30,\n",
    "        respect_robots: bool = True,\n",
    "        use_cache: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializa el descargador.\n",
    "        \n",
    "        Args:\n",
    "            delay: Segundos m√≠nimos entre peticiones al mismo dominio\n",
    "            max_retries: N√∫mero m√°ximo de reintentos por URL\n",
    "            timeout: Timeout en segundos por petici√≥n\n",
    "            respect_robots: Si verificar robots.txt antes de descargar\n",
    "            use_cache: Si cachear respuestas en memoria\n",
    "        \"\"\"\n",
    "        self.delay = delay\n",
    "        self.max_retries = max_retries\n",
    "        self.timeout = timeout\n",
    "        self.respect_robots = respect_robots\n",
    "        self.use_cache = use_cache\n",
    "        \n",
    "        # Crear sesi√≥n con reintentos configurados\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        retry_strategy = Retry(\n",
    "            total=max_retries,\n",
    "            backoff_factor=1,  # 1s, 2s, 4s entre reintentos\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        \n",
    "        # Estado interno\n",
    "        self._last_request_time: Dict[str, float] = {}  # Por dominio\n",
    "        self._cache: Dict[str, DownloadResult] = {}\n",
    "        self._robots_checker = RobotsChecker() if respect_robots else None\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        self.stats = {\n",
    "            'requests': 0,\n",
    "            'successes': 0,\n",
    "            'failures': 0,\n",
    "            'cache_hits': 0,\n",
    "        }\n",
    "    \n",
    "    def _get_domain(self, url: str) -> str:\n",
    "        \"\"\"Extrae el dominio de una URL.\"\"\"\n",
    "        return urlparse(url).netloc\n",
    "    \n",
    "    def _throttle(self, url: str):\n",
    "        \"\"\"\n",
    "        Aplica throttling por dominio.\n",
    "        \n",
    "        Espera si es necesario para respetar el delay m√≠nimo\n",
    "        entre peticiones al mismo dominio.\n",
    "        \"\"\"\n",
    "        domain = self._get_domain(url)\n",
    "        \n",
    "        if domain in self._last_request_time:\n",
    "            elapsed = time.time() - self._last_request_time[domain]\n",
    "            if elapsed < self.delay:\n",
    "                sleep_time = self.delay - elapsed\n",
    "                # A√±adir jitter aleatorio (¬±20%)\n",
    "                jitter = sleep_time * random.uniform(-0.2, 0.2)\n",
    "                time.sleep(sleep_time + jitter)\n",
    "        \n",
    "        self._last_request_time[domain] = time.time()\n",
    "    \n",
    "    def _get_headers(self) -> Dict[str, str]:\n",
    "        \"\"\"Genera headers con User-Agent aleatorio.\"\"\"\n",
    "        return {\n",
    "            'User-Agent': random.choice(self.USER_AGENTS),\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'es-MX,es;q=0.9,en;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'DNT': '1',\n",
    "        }\n",
    "    \n",
    "    def download(self, url: str) -> DownloadResult:\n",
    "        \"\"\"\n",
    "        Descarga una URL de forma robusta.\n",
    "        \n",
    "        Args:\n",
    "            url: URL a descargar\n",
    "            \n",
    "        Returns:\n",
    "            DownloadResult con el contenido o error\n",
    "        \"\"\"\n",
    "        # 1. Verificar cach√©\n",
    "        if self.use_cache and url in self._cache:\n",
    "            self.stats['cache_hits'] += 1\n",
    "            cached = self._cache[url]\n",
    "            cached.from_cache = True\n",
    "            return cached\n",
    "        \n",
    "        # 2. Verificar robots.txt\n",
    "        if self._robots_checker and not self._robots_checker.puede_acceder(url):\n",
    "            return DownloadResult(\n",
    "                url=url,\n",
    "                error=\"Bloqueado por robots.txt\"\n",
    "            )\n",
    "        \n",
    "        # 3. Aplicar throttling\n",
    "        self._throttle(url)\n",
    "        \n",
    "        # 4. Descargar\n",
    "        self.stats['requests'] += 1\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.session.get(\n",
    "                url,\n",
    "                headers=self._get_headers(),\n",
    "                timeout=self.timeout,\n",
    "                allow_redirects=True,\n",
    "            )\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            result = DownloadResult(\n",
    "                url=url,\n",
    "                status_code=response.status_code,\n",
    "                content=response.text,\n",
    "                headers=dict(response.headers),\n",
    "                elapsed=elapsed,\n",
    "            )\n",
    "            \n",
    "            if result.success:\n",
    "                self.stats['successes'] += 1\n",
    "                # Cachear respuesta exitosa\n",
    "                if self.use_cache:\n",
    "                    self._cache[url] = result\n",
    "            else:\n",
    "                self.stats['failures'] += 1\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            self.stats['failures'] += 1\n",
    "            return DownloadResult(url=url, error=\"Timeout\")\n",
    "            \n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            self.stats['failures'] += 1\n",
    "            return DownloadResult(url=url, error=f\"Error de conexi√≥n: {str(e)[:50]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.stats['failures'] += 1\n",
    "            return DownloadResult(url=url, error=f\"Error: {str(e)[:50]}\")\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Retorna estad√≠sticas de uso.\"\"\"\n",
    "        total = self.stats['requests']\n",
    "        success_rate = (self.stats['successes'] / total * 100) if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            'success_rate': f\"{success_rate:.1f}%\",\n",
    "        }\n",
    "\n",
    "\n",
    "# Demostraci√≥n\n",
    "def demostrar_downloader():\n",
    "    \"\"\"Demuestra el uso del Downloader.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"DEMOSTRACI√ìN DEL DOWNLOADER ROBUSTO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Crear downloader\n",
    "    downloader = Downloader(\n",
    "        delay=1.0,\n",
    "        max_retries=3,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    \n",
    "    # URLs de prueba\n",
    "    urls = [\n",
    "        \"https://httpbin.org/html\",\n",
    "        \"https://httpbin.org/status/404\",  # Error 404\n",
    "        \"https://httpbin.org/html\",  # Repetida (cach√©)\n",
    "    ]\n",
    "    \n",
    "    for url in urls:\n",
    "        print(f\"\\nüì• Descargando: {url}\")\n",
    "        result = downloader.download(url)\n",
    "        \n",
    "        if result.success:\n",
    "            print(f\"   ‚úÖ Status: {result.status_code}\")\n",
    "            print(f\"   üì¶ Tama√±o: {len(result.content):,} bytes\")\n",
    "            print(f\"   ‚è±Ô∏è Tiempo: {result.elapsed:.3f}s\")\n",
    "            print(f\"   üíæ Desde cach√©: {result.from_cache}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Error: {result.error or f'Status {result.status_code}'}\")\n",
    "    \n",
    "    print(f\"\\nüìä Estad√≠sticas:\")\n",
    "    for key, value in downloader.get_stats().items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "demostrar_downloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ FIN DE LA PARTE 1: FUNDAMENTOS HTTP\n",
    "\n",
    "## Resumen de Conceptos Cubiertos:\n",
    "\n",
    "1. **Anatom√≠a HTTP**: Peticiones, respuestas, headers\n",
    "2. **C√≥digos de Estado**: 2xx, 3xx, 4xx, 5xx y c√≥mo manejarlos\n",
    "3. **Sesiones y Cookies**: Mantener estado entre peticiones\n",
    "4. **robots.txt**: √âtica y verificaci√≥n de permisos\n",
    "5. **Downloader Robusto**: Reintentos, throttling, cach√©\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PARTE 2: T√âCNICAS DE EXTRACCI√ìN DE DATOS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "## 2.1 Expresiones Regulares (Regex)\n",
    "\n",
    "Las expresiones regulares son patrones que describen conjuntos de strings.\n",
    "Son extremadamente poderosas para extraer datos estructurados de texto.\n",
    "\n",
    "### Cheatsheet de Regex\n",
    "\n",
    "| Patr√≥n | Significado | Ejemplo |\n",
    "|--------|-------------|----------|\n",
    "| `.` | Cualquier car√°cter | `a.c` ‚Üí \"abc\", \"a1c\" |\n",
    "| `\\d` | D√≠gito (0-9) | `\\d{3}` ‚Üí \"123\" |\n",
    "| `\\w` | Word char (a-z, 0-9, _) | `\\w+` ‚Üí \"hello\" |\n",
    "| `\\s` | Espacio en blanco | `\\s+` ‚Üí \"   \" |\n",
    "| `*` | 0 o m√°s | `ab*` ‚Üí \"a\", \"ab\", \"abb\" |\n",
    "| `+` | 1 o m√°s | `ab+` ‚Üí \"ab\", \"abb\" |\n",
    "| `?` | 0 o 1 | `ab?` ‚Üí \"a\", \"ab\" |\n",
    "| `{n,m}` | Entre n y m | `a{2,4}` ‚Üí \"aa\", \"aaa\" |\n",
    "| `[...]` | Conjunto | `[aeiou]` ‚Üí vocales |\n",
    "| `(...)` | Grupo de captura | `(\\d+)` ‚Üí captura n√∫meros |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXTRACCI√ìN CON EXPRESIONES REGULARES\n",
      "======================================================================\n",
      "\n",
      "Texto de entrada:\n",
      "\n",
      "    Contacto: ventas@empresa.com.mx, soporte@tienda.com\n",
      "    Tel: +52 55 1234 5678, (442) 123-4567\n",
      "    Precios: $1,234.56 MXN, USD 99.99, $500\n",
      "    Visita: https://www.ejemplo.com/productos\n",
      "    Fecha: 2024-01-15, 15/03/2024\n",
      "    \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìß Emails encontrados: ['ventas@empresa.com.mx', 'soporte@tienda.com']\n",
      "üìû Tel√©fonos encontrados: ['+52 ']\n",
      "üí∞ Precios encontrados: [{'moneda': '$', 'cantidad': ','}, {'moneda': '$', 'cantidad': '52'}, {'moneda': '$', 'cantidad': '55'}, {'moneda': '$', 'cantidad': '1234'}, {'moneda': '$', 'cantidad': '5678,'}, {'moneda': '$', 'cantidad': '442'}, {'moneda': '$', 'cantidad': '123'}, {'moneda': '$', 'cantidad': '4567'}, {'moneda': '$', 'cantidad': '1,234.56'}, {'moneda': 'MXN', 'cantidad': ','}, {'moneda': 'USD', 'cantidad': '99.99'}, {'moneda': '$', 'cantidad': ','}, {'moneda': '$', 'cantidad': '500'}, {'moneda': '$', 'cantidad': '2024'}, {'moneda': '$', 'cantidad': '01'}, {'moneda': '$', 'cantidad': '15,'}, {'moneda': '$', 'cantidad': '15'}, {'moneda': '$', 'cantidad': '03'}, {'moneda': '$', 'cantidad': '2024'}]\n",
      "üîó URLs encontradas: ['https://www.ejemplo.com/productos']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2.1 EXTRACCI√ìN CON EXPRESIONES REGULARES\n",
    "# ==============================================================================\n",
    "\n",
    "class RegexExtractor:\n",
    "    \"\"\"\n",
    "    Extractor de datos usando expresiones regulares.\n",
    "    \n",
    "    Incluye patrones pre-compilados para los casos m√°s comunes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Patrones pre-compilados para mejor rendimiento\n",
    "    PATTERNS = {\n",
    "        # Email: usuario@dominio.ext\n",
    "        # [a-zA-Z0-9._%+-]+ : parte local (letras, n√∫meros, puntos, etc.)\n",
    "        # @                 : arroba literal\n",
    "        # [a-zA-Z0-9.-]+    : dominio\n",
    "        # \\.[a-zA-Z]{2,}    : extensi√≥n (.com, .org, .mx)\n",
    "        'email': re.compile(\n",
    "            r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
    "            re.IGNORECASE\n",
    "        ),\n",
    "        \n",
    "        # Tel√©fono mexicano: +52 55 1234 5678 o variantes\n",
    "        # (\\+52\\s*)?        : c√≥digo de pa√≠s opcional\n",
    "        # \\(?\\d{2,3}\\)?    : c√≥digo de √°rea (55, 442, etc.)\n",
    "        # [\\s.-]?          : separador opcional\n",
    "        # \\d{4}[\\s.-]?\\d{4}: 8 d√≠gitos con separador opcional\n",
    "        'telefono_mx': re.compile(\n",
    "            r'(\\+52\\s*)?\\(?\\d{2,3}\\)?[\\s.-]?\\d{4}[\\s.-]?\\d{4}',\n",
    "            re.IGNORECASE\n",
    "        ),\n",
    "        \n",
    "        # Precio: $1,234.56 o variantes\n",
    "        # (?P<moneda>...)   : grupo nombrado para moneda\n",
    "        # [\\d,]+            : d√≠gitos y comas\n",
    "        # (?:\\.\\d{2})?     : decimales opcionales\n",
    "        'precio': re.compile(\n",
    "            r'(?P<moneda>\\$|USD|MXN)?\\s*(?P<cantidad>[\\d,]+(?:\\.\\d{2})?)',\n",
    "            re.IGNORECASE\n",
    "        ),\n",
    "        \n",
    "        # URL: http(s)://dominio.com/path\n",
    "        'url': re.compile(\n",
    "            r'https?://[\\w.-]+(?:/[\\w./-]*)?',\n",
    "            re.IGNORECASE\n",
    "        ),\n",
    "        \n",
    "        # Fecha ISO: 2024-01-15\n",
    "        'fecha_iso': re.compile(r'\\d{4}-\\d{2}-\\d{2}'),\n",
    "        \n",
    "        # Fecha espa√±ol: 15/03/2024\n",
    "        'fecha_es': re.compile(r'\\d{1,2}/\\d{1,2}/\\d{4}'),\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def extraer_emails(cls, texto: str) -> List[str]:\n",
    "        \"\"\"Extrae todos los emails del texto.\"\"\"\n",
    "        return cls.PATTERNS['email'].findall(texto)\n",
    "    \n",
    "    @classmethod\n",
    "    def extraer_telefonos(cls, texto: str) -> List[str]:\n",
    "        \"\"\"Extrae tel√©fonos mexicanos del texto.\"\"\"\n",
    "        return cls.PATTERNS['telefono_mx'].findall(texto)\n",
    "    \n",
    "    @classmethod\n",
    "    def extraer_precios(cls, texto: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extrae precios con su moneda.\"\"\"\n",
    "        matches = cls.PATTERNS['precio'].finditer(texto)\n",
    "        return [\n",
    "            {'moneda': m.group('moneda') or '$', 'cantidad': m.group('cantidad')}\n",
    "            for m in matches\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def extraer_urls(cls, texto: str) -> List[str]:\n",
    "        \"\"\"Extrae URLs del texto.\"\"\"\n",
    "        return cls.PATTERNS['url'].findall(texto)\n",
    "\n",
    "\n",
    "# Demostraci√≥n\n",
    "def demostrar_regex():\n",
    "    \"\"\"Demuestra extracci√≥n con regex.\"\"\"\n",
    "    \n",
    "    texto_ejemplo = \"\"\"\n",
    "    Contacto: ventas@empresa.com.mx, soporte@tienda.com\n",
    "    Tel: +52 55 1234 5678, (442) 123-4567\n",
    "    Precios: $1,234.56 MXN, USD 99.99, $500\n",
    "    Visita: https://www.ejemplo.com/productos\n",
    "    Fecha: 2024-01-15, 15/03/2024\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"EXTRACCI√ìN CON EXPRESIONES REGULARES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTexto de entrada:\\n{texto_ejemplo}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    print(f\"\\nüìß Emails encontrados: {RegexExtractor.extraer_emails(texto_ejemplo)}\")\n",
    "    print(f\"üìû Tel√©fonos encontrados: {RegexExtractor.extraer_telefonos(texto_ejemplo)}\")\n",
    "    print(f\"üí∞ Precios encontrados: {RegexExtractor.extraer_precios(texto_ejemplo)}\")\n",
    "    print(f\"üîó URLs encontradas: {RegexExtractor.extraer_urls(texto_ejemplo)}\")\n",
    "\n",
    "demostrar_regex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 BeautifulSoup: El Parser Amigable\n",
    "\n",
    "BeautifulSoup es la biblioteca m√°s popular para parsing HTML en Python.\n",
    "Su API es intuitiva y f√°cil de usar.\n",
    "\n",
    "### M√©todos Principales\n",
    "\n",
    "| M√©todo | Descripci√≥n | Ejemplo |\n",
    "|--------|-------------|----------|\n",
    "| `find()` | Primer elemento que coincide | `soup.find('div', class_='item')` |\n",
    "| `find_all()` | Todos los elementos | `soup.find_all('a')` |\n",
    "| `select()` | CSS selector (m√∫ltiples) | `soup.select('div.item > p')` |\n",
    "| `select_one()` | CSS selector (primero) | `soup.select_one('#main')` |\n",
    "| `.text` | Texto del elemento | `elem.text` |\n",
    "| `.get()` | Atributo del elemento | `elem.get('href')` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BEAUTIFULSOUP - EXTRACCI√ìN DE DATOS HTML\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ find() - Encontrar primer elemento\n",
      "   T√≠tulo: Tienda de Electr√≥nica\n",
      "   Primer producto: Laptop HP\n",
      "   Header H1: Mi Tienda\n",
      "\n",
      "2Ô∏è‚É£ find_all() - Encontrar todos\n",
      "   Total productos: 3\n",
      "   Total links: 6\n",
      "      - Inicio: /inicio\n",
      "      - Productos: /productos\n",
      "      - Contacto: /contacto\n",
      "\n",
      "3Ô∏è‚É£ select() - CSS Selectors\n",
      "   Precios: ['$15,999.00', '$4,599.00', '$1,299.00']\n",
      "   Nombres: ['Laptop HP', 'Monitor Samsung', 'Teclado Mec√°nico']\n",
      "   Producto destacado: Teclado Mec√°nico\n",
      "\n",
      "4Ô∏è‚É£ Extraer atributos\n",
      "   ID:001 | Laptop HP | /producto/001\n",
      "   ID:002 | Monitor Samsung | /producto/002\n",
      "   ID:003 | Teclado Mec√°nico | /producto/003\n",
      "\n",
      "5Ô∏è‚É£ Extraer tabla\n",
      "   Procesador: Intel i5\n",
      "   RAM: 8 GB\n",
      "   Almacenamiento: 256 GB SSD\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2.2 BEAUTIFULSOUP - EXTRACCI√ìN DE DATOS HTML\n",
    "# ==============================================================================\n",
    "\n",
    "# HTML de ejemplo para practicar\n",
    "HTML_EJEMPLO = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>Tienda de Electr√≥nica</title></head>\n",
    "<body>\n",
    "    <div id=\"header\">\n",
    "        <h1>Mi Tienda</h1>\n",
    "        <nav>\n",
    "            <a href=\"/inicio\">Inicio</a>\n",
    "            <a href=\"/productos\">Productos</a>\n",
    "            <a href=\"/contacto\">Contacto</a>\n",
    "        </nav>\n",
    "    </div>\n",
    "    \n",
    "    <div id=\"productos\">\n",
    "        <div class=\"producto\" data-id=\"001\">\n",
    "            <h2 class=\"nombre\">Laptop HP</h2>\n",
    "            <span class=\"precio\">$15,999.00</span>\n",
    "            <p class=\"descripcion\">Laptop HP con procesador Intel i5</p>\n",
    "            <a href=\"/producto/001\" class=\"ver-mas\">Ver m√°s</a>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"producto\" data-id=\"002\">\n",
    "            <h2 class=\"nombre\">Monitor Samsung</h2>\n",
    "            <span class=\"precio\">$4,599.00</span>\n",
    "            <p class=\"descripcion\">Monitor 24 pulgadas Full HD</p>\n",
    "            <a href=\"/producto/002\" class=\"ver-mas\">Ver m√°s</a>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"producto destacado\" data-id=\"003\">\n",
    "            <h2 class=\"nombre\">Teclado Mec√°nico</h2>\n",
    "            <span class=\"precio\">$1,299.00</span>\n",
    "            <span class=\"descuento\">-20%</span>\n",
    "            <p class=\"descripcion\">Teclado mec√°nico RGB</p>\n",
    "            <a href=\"/producto/003\" class=\"ver-mas\">Ver m√°s</a>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <table id=\"especificaciones\">\n",
    "        <tr><th>Caracter√≠stica</th><th>Valor</th></tr>\n",
    "        <tr><td>Procesador</td><td>Intel i5</td></tr>\n",
    "        <tr><td>RAM</td><td>8 GB</td></tr>\n",
    "        <tr><td>Almacenamiento</td><td>256 GB SSD</td></tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "def demostrar_beautifulsoup():\n",
    "    \"\"\"Demuestra todas las t√©cnicas de BeautifulSoup.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"BEAUTIFULSOUP - EXTRACCI√ìN DE DATOS HTML\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Crear objeto BeautifulSoup\n",
    "    # 'lxml' es el parser m√°s r√°pido (requiere: pip install lxml)\n",
    "    soup = BeautifulSoup(HTML_EJEMPLO, 'lxml')\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 1. find() - Encontrar un elemento\n",
    "    # =========================================================================\n",
    "    print(\"\\n1Ô∏è‚É£ find() - Encontrar primer elemento\")\n",
    "    \n",
    "    # Por tag\n",
    "    titulo = soup.find('title')\n",
    "    print(f\"   T√≠tulo: {titulo.text}\")\n",
    "    \n",
    "    # Por clase\n",
    "    primer_producto = soup.find('div', class_='producto')\n",
    "    print(f\"   Primer producto: {primer_producto.find('h2').text}\")\n",
    "    \n",
    "    # Por ID\n",
    "    header = soup.find(id='header')\n",
    "    print(f\"   Header H1: {header.find('h1').text}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 2. find_all() - Encontrar todos los elementos\n",
    "    # =========================================================================\n",
    "    print(\"\\n2Ô∏è‚É£ find_all() - Encontrar todos\")\n",
    "    \n",
    "    # Todos los productos\n",
    "    productos = soup.find_all('div', class_='producto')\n",
    "    print(f\"   Total productos: {len(productos)}\")\n",
    "    \n",
    "    # Todos los links\n",
    "    links = soup.find_all('a')\n",
    "    print(f\"   Total links: {len(links)}\")\n",
    "    for link in links[:3]:\n",
    "        print(f\"      - {link.text}: {link.get('href')}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 3. select() - CSS Selectors\n",
    "    # =========================================================================\n",
    "    print(\"\\n3Ô∏è‚É£ select() - CSS Selectors\")\n",
    "    \n",
    "    # Selector de clase\n",
    "    precios = soup.select('.precio')\n",
    "    print(f\"   Precios: {[p.text for p in precios]}\")\n",
    "    \n",
    "    # Selector combinado\n",
    "    nombres = soup.select('div.producto > h2.nombre')\n",
    "    print(f\"   Nombres: {[n.text for n in nombres]}\")\n",
    "    \n",
    "    # Producto destacado\n",
    "    destacado = soup.select_one('div.producto.destacado')\n",
    "    print(f\"   Producto destacado: {destacado.find('h2').text if destacado else 'Ninguno'}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 4. Extraer atributos\n",
    "    # =========================================================================\n",
    "    print(\"\\n4Ô∏è‚É£ Extraer atributos\")\n",
    "    \n",
    "    for prod in productos:\n",
    "        data_id = prod.get('data-id')\n",
    "        nombre = prod.find('h2').text\n",
    "        link = prod.find('a').get('href')\n",
    "        print(f\"   ID:{data_id} | {nombre} | {link}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 5. Extraer tabla\n",
    "    # =========================================================================\n",
    "    print(\"\\n5Ô∏è‚É£ Extraer tabla\")\n",
    "    \n",
    "    tabla = soup.find('table', id='especificaciones')\n",
    "    filas = tabla.find_all('tr')[1:]  # Omitir header\n",
    "    \n",
    "    for fila in filas:\n",
    "        celdas = fila.find_all('td')\n",
    "        caracteristica = celdas[0].text\n",
    "        valor = celdas[1].text\n",
    "        print(f\"   {caracteristica}: {valor}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "demostrar_beautifulsoup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 lxml y XPath: M√°ximo Rendimiento\n",
    "\n",
    "lxml es significativamente m√°s r√°pido que BeautifulSoup y soporta XPath completo.\n",
    "\n",
    "### XPath Cheatsheet\n",
    "\n",
    "| Expresi√≥n | Significado |\n",
    "|-----------|-------------|\n",
    "| `//div` | Todos los div en cualquier nivel |\n",
    "| `/html/body/div` | Path absoluto |\n",
    "| `//div[@class='item']` | div con clase 'item' |\n",
    "| `//div[@id='main']` | div con id 'main' |\n",
    "| `//a/@href` | Atributo href de todos los links |\n",
    "| `//p/text()` | Texto de todos los p√°rrafos |\n",
    "| `//div[contains(@class, 'item')]` | Clase contiene 'item' |\n",
    "| `//div[1]` | Primer div |\n",
    "| `//div[last()]` | √öltimo div |\n",
    "| `//div[position()<=3]` | Primeros 3 divs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LXML Y XPATH - ALTO RENDIMIENTO\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ XPath b√°sico\n",
      "   T√≠tulo: Tienda de Electr√≥nica\n",
      "   Productos: ['Laptop HP', 'Monitor Samsung', 'Teclado Mec√°nico']\n",
      "\n",
      "2Ô∏è‚É£ XPath con predicados\n",
      "   Producto 002: Monitor Samsung\n",
      "   Destacado: Teclado Mec√°nico\n",
      "\n",
      "3Ô∏è‚É£ Extraer atributos\n",
      "   Links: ['/inicio', '/productos', '/contacto', '/producto/001', '/producto/002', '/producto/003']\n",
      "\n",
      "4Ô∏è‚É£ Extracci√≥n estructurada de productos\n",
      "   {'id': '001', 'nombre': 'Laptop HP', 'precio': '$15,999.00', 'link': '/producto/001', 'descuento': None}\n",
      "   {'id': '002', 'nombre': 'Monitor Samsung', 'precio': '$4,599.00', 'link': '/producto/002', 'descuento': None}\n",
      "   {'id': '003', 'nombre': 'Teclado Mec√°nico', 'precio': '$1,299.00', 'link': '/producto/003', 'descuento': '-20%'}\n",
      "\n",
      "5Ô∏è‚É£ Comparaci√≥n de rendimiento\n",
      "   BeautifulSoup: 0.0442s para 100 iteraciones\n",
      "   lxml/XPath:    0.0033s para 100 iteraciones\n",
      "   lxml es 13.3x m√°s r√°pido\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2.3 LXML Y XPATH - EXTRACCI√ìN DE ALTO RENDIMIENTO\n",
    "# ==============================================================================\n",
    "\n",
    "def demostrar_lxml_xpath():\n",
    "    \"\"\"Demuestra extracci√≥n con lxml y XPath.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"LXML Y XPATH - ALTO RENDIMIENTO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Parsear HTML con lxml\n",
    "    tree = lxml_html.fromstring(HTML_EJEMPLO)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 1. XPath b√°sico\n",
    "    # =========================================================================\n",
    "    print(\"\\n1Ô∏è‚É£ XPath b√°sico\")\n",
    "    \n",
    "    # T√≠tulo\n",
    "    titulo = tree.xpath('//title/text()')\n",
    "    print(f\"   T√≠tulo: {titulo[0] if titulo else 'N/A'}\")\n",
    "    \n",
    "    # Todos los nombres de productos\n",
    "    nombres = tree.xpath('//h2[@class=\"nombre\"]/text()')\n",
    "    print(f\"   Productos: {nombres}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 2. XPath con predicados\n",
    "    # =========================================================================\n",
    "    print(\"\\n2Ô∏è‚É£ XPath con predicados\")\n",
    "    \n",
    "    # Producto con data-id espec√≠fico\n",
    "    producto_002 = tree.xpath('//div[@data-id=\"002\"]//h2/text()')\n",
    "    print(f\"   Producto 002: {producto_002[0] if producto_002 else 'N/A'}\")\n",
    "    \n",
    "    # Producto destacado\n",
    "    destacado = tree.xpath('//div[contains(@class, \"destacado\")]//h2/text()')\n",
    "    print(f\"   Destacado: {destacado[0] if destacado else 'N/A'}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 3. Extraer atributos\n",
    "    # =========================================================================\n",
    "    print(\"\\n3Ô∏è‚É£ Extraer atributos\")\n",
    "    \n",
    "    # Todos los hrefs\n",
    "    hrefs = tree.xpath('//a/@href')\n",
    "    print(f\"   Links: {hrefs}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 4. Extracci√≥n estructurada\n",
    "    # =========================================================================\n",
    "    print(\"\\n4Ô∏è‚É£ Extracci√≥n estructurada de productos\")\n",
    "    \n",
    "    productos = tree.xpath('//div[@class=\"producto\" or contains(@class, \"producto \")]')\n",
    "    \n",
    "    datos_productos = []\n",
    "    for prod in productos:\n",
    "        # Usar XPath relativo (.//) dentro del elemento\n",
    "        nombre = prod.xpath('.//h2[@class=\"nombre\"]/text()')\n",
    "        precio = prod.xpath('.//span[@class=\"precio\"]/text()')\n",
    "        data_id = prod.xpath('@data-id')\n",
    "        link = prod.xpath('.//a/@href')\n",
    "        descuento = prod.xpath('.//span[@class=\"descuento\"]/text()')\n",
    "        \n",
    "        producto = {\n",
    "            'id': data_id[0] if data_id else None,\n",
    "            'nombre': nombre[0] if nombre else None,\n",
    "            'precio': precio[0] if precio else None,\n",
    "            'link': link[0] if link else None,\n",
    "            'descuento': descuento[0] if descuento else None,\n",
    "        }\n",
    "        datos_productos.append(producto)\n",
    "        print(f\"   {producto}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 5. Comparaci√≥n de rendimiento\n",
    "    # =========================================================================\n",
    "    print(\"\\n5Ô∏è‚É£ Comparaci√≥n de rendimiento\")\n",
    "    \n",
    "    import timeit\n",
    "    \n",
    "    # BeautifulSoup\n",
    "    def bs_parse():\n",
    "        soup = BeautifulSoup(HTML_EJEMPLO, 'lxml')\n",
    "        return soup.find_all('div', class_='producto')\n",
    "    \n",
    "    # lxml\n",
    "    def lxml_parse():\n",
    "        tree = lxml_html.fromstring(HTML_EJEMPLO)\n",
    "        return tree.xpath('//div[contains(@class, \"producto\")]')\n",
    "    \n",
    "    n = 100\n",
    "    bs_time = timeit.timeit(bs_parse, number=n)\n",
    "    lxml_time = timeit.timeit(lxml_parse, number=n)\n",
    "    \n",
    "    print(f\"   BeautifulSoup: {bs_time:.4f}s para {n} iteraciones\")\n",
    "    print(f\"   lxml/XPath:    {lxml_time:.4f}s para {n} iteraciones\")\n",
    "    print(f\"   lxml es {bs_time/lxml_time:.1f}x m√°s r√°pido\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    return datos_productos\n",
    "\n",
    "productos_extraidos = demostrar_lxml_xpath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PARTE 3: CRAWLERS - NAVEGACI√ìN AUTOM√ÅTICA\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "## 3.1 ¬øQu√© es un Crawler?\n",
    "\n",
    "Un **crawler** (ara√±a web) es un programa que navega autom√°ticamente por la web\n",
    "siguiendo enlaces de p√°gina en p√°gina.\n",
    "\n",
    "### Crawler vs Scraper\n",
    "\n",
    "| Aspecto | Scraper | Crawler |\n",
    "|---------|---------|----------|\n",
    "| URLs | Conocidas de antemano | Descubiertas din√°micamente |\n",
    "| Alcance | P√°ginas espec√≠ficas | Sitio completo/parcial |\n",
    "| Complejidad | Menor | Mayor |\n",
    "| Riesgo de loops | Bajo | Alto (necesita control) |\n",
    "\n",
    "### Algoritmos de Crawling\n",
    "\n",
    "```\n",
    "BFS (Breadth-First Search)          DFS (Depth-First Search)\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "         A                                    A\n",
    "        /|\\                                  /|\\\n",
    "       B C D    ‚Üê Nivel 1                   B C D\n",
    "      /|   |                               /|\n",
    "     E F   G    ‚Üê Nivel 2                 E F\n",
    "\n",
    "Orden: A‚ÜíB‚ÜíC‚ÜíD‚ÜíE‚ÜíF‚ÜíG                 Orden: A‚ÜíB‚ÜíE‚ÜíF‚ÜíC‚ÜíD‚ÜíG\n",
    "(Por niveles)                       (En profundidad)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Estructuras de datos para crawling definidas\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3.1 ESTRUCTURAS DE DATOS PARA CRAWLING\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class CrawlResult:\n",
    "    \"\"\"\n",
    "    Resultado de crawlear una URL.\n",
    "    \n",
    "    Attributes:\n",
    "        url: URL visitada\n",
    "        status_code: C√≥digo HTTP\n",
    "        content: Contenido HTML\n",
    "        links_found: Enlaces encontrados\n",
    "        depth: Profundidad desde la semilla\n",
    "        elapsed: Tiempo de descarga\n",
    "        error: Mensaje de error si fall√≥\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    status_code: Optional[int] = None\n",
    "    content: Optional[str] = None\n",
    "    links_found: List[str] = field(default_factory=list)\n",
    "    depth: int = 0\n",
    "    elapsed: float = 0.0\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def success(self) -> bool:\n",
    "        return self.status_code is not None and 200 <= self.status_code < 300\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CrawlerStats:\n",
    "    \"\"\"Estad√≠sticas del crawler.\"\"\"\n",
    "    pages_crawled: int = 0\n",
    "    pages_success: int = 0\n",
    "    pages_failed: int = 0\n",
    "    links_found: int = 0\n",
    "    start_time: Optional[datetime] = None\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> float:\n",
    "        if not self.start_time:\n",
    "            return 0.0\n",
    "        return (datetime.now() - self.start_time).total_seconds()\n",
    "    \n",
    "    @property\n",
    "    def pages_per_second(self) -> float:\n",
    "        return self.pages_crawled / self.duration if self.duration > 0 else 0\n",
    "\n",
    "print(\"‚úÖ Estructuras de datos para crawling definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clase BFSCrawler definida\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3.2 CRAWLER BFS (BREADTH-FIRST SEARCH)\n",
    "# ==============================================================================\n",
    "\n",
    "class BFSCrawler:\n",
    "    \"\"\"\n",
    "    Crawler que usa Breadth-First Search.\n",
    "    \n",
    "    Explora el sitio por niveles: primero todas las p√°ginas a profundidad 1,\n",
    "    luego todas a profundidad 2, etc.\n",
    "    \n",
    "    Usa una COLA (FIFO) para mantener el orden.\n",
    "    \n",
    "    Example:\n",
    "        >>> crawler = BFSCrawler(\"https://ejemplo.com\", max_pages=50)\n",
    "        >>> for result in crawler.crawl():\n",
    "        ...     print(f\"Visitado: {result.url}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        seed_url: str,\n",
    "        max_pages: int = 100,\n",
    "        max_depth: int = 5,\n",
    "        delay: float = 1.0,\n",
    "        same_domain: bool = True,\n",
    "    ):\n",
    "        self.seed_url = seed_url\n",
    "        self.max_pages = max_pages\n",
    "        self.max_depth = max_depth\n",
    "        self.delay = delay\n",
    "        self.same_domain = same_domain\n",
    "        \n",
    "        # Extraer dominio semilla\n",
    "        parsed = urlparse(seed_url)\n",
    "        self.seed_domain = parsed.netloc\n",
    "        \n",
    "        # Estado del crawler\n",
    "        self.frontier: deque = deque()  # Cola FIFO para BFS\n",
    "        self.visited: Set[str] = set()  # URLs ya visitadas\n",
    "        self.stats = CrawlerStats()\n",
    "        \n",
    "        # Sesi√≥n HTTP\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'BFSCrawler/1.0 (Educational)',\n",
    "        })\n",
    "    \n",
    "    def _normalize_url(self, url: str) -> str:\n",
    "        \"\"\"Normaliza URL para evitar duplicados.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        # Eliminar fragment y normalizar\n",
    "        path = parsed.path.rstrip('/') if parsed.path != '/' else '/'\n",
    "        return urlunparse((\n",
    "            parsed.scheme, parsed.netloc.lower(), path,\n",
    "            '', parsed.query, ''\n",
    "        ))\n",
    "    \n",
    "    def _is_valid_url(self, url: str, depth: int) -> bool:\n",
    "        \"\"\"Verifica si debemos visitar esta URL.\"\"\"\n",
    "        if url in self.visited:\n",
    "            return False\n",
    "        if depth > self.max_depth:\n",
    "            return False\n",
    "        \n",
    "        parsed = urlparse(url)\n",
    "        \n",
    "        if parsed.scheme not in ('http', 'https'):\n",
    "            return False\n",
    "        \n",
    "        if self.same_domain and parsed.netloc != self.seed_domain:\n",
    "            return False\n",
    "        \n",
    "        # Ignorar archivos no-HTML\n",
    "        blocked_ext = {'.jpg', '.png', '.gif', '.pdf', '.zip', '.css', '.js'}\n",
    "        if any(parsed.path.lower().endswith(ext) for ext in blocked_ext):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _extract_links(self, html: str, base_url: str) -> List[str]:\n",
    "        \"\"\"Extrae y normaliza enlaces de una p√°gina.\"\"\"\n",
    "        links = []\n",
    "        try:\n",
    "            tree = lxml_html.fromstring(html)\n",
    "            hrefs = tree.xpath('//a/@href')\n",
    "            \n",
    "            for href in hrefs:\n",
    "                if not href or href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                    continue\n",
    "                absolute = urljoin(base_url, href)\n",
    "                normalized = self._normalize_url(absolute)\n",
    "                if normalized not in links:\n",
    "                    links.append(normalized)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return links\n",
    "    \n",
    "    def _download(self, url: str) -> Tuple[Optional[str], Optional[int], Optional[str]]:\n",
    "        \"\"\"Descarga una URL.\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            if 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "                return None, response.status_code, \"No es HTML\"\n",
    "            return response.text, response.status_code, None\n",
    "        except requests.exceptions.Timeout:\n",
    "            return None, None, \"Timeout\"\n",
    "        except Exception as e:\n",
    "            return None, None, str(e)\n",
    "    \n",
    "    def crawl(self) -> Generator[CrawlResult, None, None]:\n",
    "        \"\"\"\n",
    "        Ejecuta el crawling BFS.\n",
    "        \n",
    "        Yields:\n",
    "            CrawlResult para cada p√°gina visitada\n",
    "        \"\"\"\n",
    "        self.stats.start_time = datetime.now()\n",
    "        \n",
    "        # Inicializar con semilla\n",
    "        self.frontier.append((self.seed_url, 0))  # (url, depth)\n",
    "        self.visited.add(self.seed_url)\n",
    "        \n",
    "        while self.frontier and self.stats.pages_crawled < self.max_pages:\n",
    "            # BFS: popleft() - FIFO\n",
    "            url, depth = self.frontier.popleft()\n",
    "            \n",
    "            # Descargar\n",
    "            start = time.time()\n",
    "            content, status, error = self._download(url)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            result = CrawlResult(\n",
    "                url=url, status_code=status, content=content,\n",
    "                depth=depth, elapsed=elapsed, error=error\n",
    "            )\n",
    "            \n",
    "            self.stats.pages_crawled += 1\n",
    "            if result.success:\n",
    "                self.stats.pages_success += 1\n",
    "                \n",
    "                # Extraer enlaces\n",
    "                links = self._extract_links(content, url)\n",
    "                result.links_found = links\n",
    "                self.stats.links_found += len(links)\n",
    "                \n",
    "                # A√±adir enlaces v√°lidos al frontier\n",
    "                for link in links:\n",
    "                    if self._is_valid_url(link, depth + 1):\n",
    "                        self.frontier.append((link, depth + 1))\n",
    "                        self.visited.add(link)\n",
    "            else:\n",
    "                self.stats.pages_failed += 1\n",
    "            \n",
    "            yield result\n",
    "            \n",
    "            # Throttling\n",
    "            if self.frontier:\n",
    "                time.sleep(self.delay)\n",
    "\n",
    "print(\"‚úÖ Clase BFSCrawler definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARACI√ìN BFS vs DFS\n",
      "======================================================================\n",
      "\n",
      "üìä Estructura del sitio:\n",
      "   home ‚Üí [about, products, contact]\n",
      "   about ‚Üí [team, history]\n",
      "   products ‚Üí [product1, product2]\n",
      "\n",
      "üîÑ BFS (por niveles):\n",
      "   1. D0: home\n",
      "   2. D1: about\n",
      "   3. D1: products\n",
      "   4. D1: contact\n",
      "   5. D2: team\n",
      "   6. D2: history\n",
      "   7. D2: product1\n",
      "   8. D2: product2\n",
      "   9. D2: form\n",
      "\n",
      "üîΩ DFS (en profundidad):\n",
      "   1. D0: home\n",
      "   2. D1: contact\n",
      "   3. D2: form\n",
      "   4. D1: products\n",
      "   5. D2: product2\n",
      "   6. D2: product1\n",
      "   7. D1: about\n",
      "   8. D2: history\n",
      "   9. D2: team\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3.3 CRAWLER DFS (DEPTH-FIRST SEARCH)\n",
    "# ==============================================================================\n",
    "\n",
    "class DFSCrawler(BFSCrawler):\n",
    "    \"\"\"\n",
    "    Crawler que usa Depth-First Search.\n",
    "    \n",
    "    La √∫nica diferencia con BFS es usar pop() en lugar de popleft().\n",
    "    Esto lo convierte en LIFO (pila) en lugar de FIFO (cola).\n",
    "    \"\"\"\n",
    "    \n",
    "    def crawl(self) -> Generator[CrawlResult, None, None]:\n",
    "        \"\"\"Ejecuta crawling DFS.\"\"\"\n",
    "        self.stats.start_time = datetime.now()\n",
    "        \n",
    "        self.frontier.append((self.seed_url, 0))\n",
    "        self.visited.add(self.seed_url)\n",
    "        \n",
    "        while self.frontier and self.stats.pages_crawled < self.max_pages:\n",
    "            # DFS: pop() - LIFO (la √∫nica diferencia!)\n",
    "            url, depth = self.frontier.pop()\n",
    "            \n",
    "            start = time.time()\n",
    "            content, status, error = self._download(url)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            result = CrawlResult(\n",
    "                url=url, status_code=status, content=content,\n",
    "                depth=depth, elapsed=elapsed, error=error\n",
    "            )\n",
    "            \n",
    "            self.stats.pages_crawled += 1\n",
    "            if result.success:\n",
    "                self.stats.pages_success += 1\n",
    "                links = self._extract_links(content, url)\n",
    "                result.links_found = links\n",
    "                \n",
    "                for link in links:\n",
    "                    if self._is_valid_url(link, depth + 1):\n",
    "                        self.frontier.append((link, depth + 1))\n",
    "                        self.visited.add(link)\n",
    "            else:\n",
    "                self.stats.pages_failed += 1\n",
    "            \n",
    "            yield result\n",
    "            \n",
    "            if self.frontier:\n",
    "                time.sleep(self.delay)\n",
    "\n",
    "\n",
    "# Demostraci√≥n BFS vs DFS\n",
    "def demo_bfs_vs_dfs():\n",
    "    \"\"\"Demuestra la diferencia entre BFS y DFS.\"\"\"\n",
    "    \n",
    "    # Simular grafo de p√°ginas\n",
    "    grafo = {\n",
    "        'home': ['about', 'products', 'contact'],\n",
    "        'about': ['team', 'history'],\n",
    "        'products': ['product1', 'product2'],\n",
    "        'contact': ['form'],\n",
    "        'team': [], 'history': [], 'product1': [], 'product2': [], 'form': []\n",
    "    }\n",
    "    \n",
    "    def simular(estrategia):\n",
    "        if estrategia == 'bfs':\n",
    "            frontier = deque([('home', 0)])\n",
    "        else:\n",
    "            frontier = [('home', 0)]\n",
    "        \n",
    "        visited = {'home'}\n",
    "        orden = []\n",
    "        \n",
    "        while frontier:\n",
    "            if estrategia == 'bfs':\n",
    "                pagina, depth = frontier.popleft()\n",
    "            else:\n",
    "                pagina, depth = frontier.pop()\n",
    "            \n",
    "            orden.append((pagina, depth))\n",
    "            \n",
    "            for enlace in grafo.get(pagina, []):\n",
    "                if enlace not in visited:\n",
    "                    visited.add(enlace)\n",
    "                    if estrategia == 'bfs':\n",
    "                        frontier.append((enlace, depth + 1))\n",
    "                    else:\n",
    "                        frontier.append((enlace, depth + 1))\n",
    "        return orden\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPARACI√ìN BFS vs DFS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nüìä Estructura del sitio:\")\n",
    "    print(\"   home ‚Üí [about, products, contact]\")\n",
    "    print(\"   about ‚Üí [team, history]\")\n",
    "    print(\"   products ‚Üí [product1, product2]\")\n",
    "    \n",
    "    print(\"\\nüîÑ BFS (por niveles):\")\n",
    "    for i, (p, d) in enumerate(simular('bfs'), 1):\n",
    "        print(f\"   {i}. D{d}: {p}\")\n",
    "    \n",
    "    print(\"\\nüîΩ DFS (en profundidad):\")\n",
    "    for i, (p, d) in enumerate(simular('dfs'), 1):\n",
    "        print(f\"   {i}. D{d}: {p}\")\n",
    "\n",
    "demo_bfs_vs_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PARTE 4: T√âCNICAS ANTI-BAN Y EVASI√ìN DE DETECCI√ìN\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "## ‚ö†Ô∏è Advertencia √âtica\n",
    "Este material es para fines educativos. √ösalo responsablemente.\n",
    "\n",
    "## 4.1 ¬øC√≥mo Detectan los Sitios a los Bots?\n",
    "\n",
    "### Niveles de Detecci√≥n\n",
    "\n",
    "| Nivel | T√©cnica | Soluci√≥n |\n",
    "|-------|---------|----------|\n",
    "| **B√°sico** | User-Agent sospechoso | User-Agent realista |\n",
    "| **B√°sico** | Velocidad excesiva | Throttling con delays |\n",
    "| **Intermedio** | Rate limiting por IP | Rotaci√≥n de proxies |\n",
    "| **Intermedio** | Headers faltantes | Headers completos |\n",
    "| **Avanzado** | JavaScript fingerprinting | Selenium/Playwright |\n",
    "| **Avanzado** | Behavioral analysis | Simular comportamiento humano |\n",
    "| **Avanzado** | CAPTCHAs | Servicios de resoluci√≥n |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SIMULADOR DE COMPORTAMIENTO HUMANO\n",
      "======================================================================\n",
      "\n",
      "Simulando 5 delays humanos:\n",
      "   Delay 1: 0.97s\n",
      "   Delay 2: 1.91s\n",
      "   Delay 3: 1.70s\n",
      "   Delay 4: 1.53s\n",
      "   Delay 5: 4.10s\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4.1 SIMULADOR DE COMPORTAMIENTO HUMANO\n",
    "# ==============================================================================\n",
    "\n",
    "class HumanBehaviorSimulator:\n",
    "    \"\"\"\n",
    "    Simula comportamiento humano para evitar detecci√≥n.\n",
    "    \n",
    "    Los bots tienen patrones predecibles:\n",
    "    - Timing exacto entre peticiones\n",
    "    - Sin variaci√≥n en comportamiento\n",
    "    \n",
    "    Los humanos son impredecibles:\n",
    "    - Pausas para leer\n",
    "    - Velocidad variable\n",
    "    - Breaks ocasionales\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        min_delay: float = 1.0,\n",
    "        max_delay: float = 5.0,\n",
    "        break_probability: float = 0.05,\n",
    "    ):\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.break_probability = break_probability\n",
    "        self.requests_count = 0\n",
    "    \n",
    "    def wait_human_like(self):\n",
    "        \"\"\"\n",
    "        Espera un tiempo que simula comportamiento humano.\n",
    "        \n",
    "        Usa distribuci√≥n log-normal que produce:\n",
    "        - Muchos delays cortos\n",
    "        - Algunos delays largos ocasionales\n",
    "        \"\"\"\n",
    "        # Delay base con distribuci√≥n log-normal\n",
    "        delay = random.lognormvariate(0.5, 0.5)\n",
    "        delay = self.min_delay + (delay * (self.max_delay - self.min_delay) / 3)\n",
    "        delay = max(self.min_delay, min(self.max_delay * 1.5, delay))\n",
    "        \n",
    "        # Ocasionalmente simular \"tiempo de lectura\"\n",
    "        if random.random() < 0.2:\n",
    "            delay += random.uniform(2.0, 8.0)\n",
    "        \n",
    "        time.sleep(delay)\n",
    "        self.requests_count += 1\n",
    "    \n",
    "    def should_take_break(self) -> bool:\n",
    "        \"\"\"Determina si tomar un descanso.\"\"\"\n",
    "        # Probabilidad aumenta con n√∫mero de requests\n",
    "        adjusted_prob = self.break_probability * (1 + self.requests_count * 0.01)\n",
    "        return random.random() < adjusted_prob\n",
    "    \n",
    "    def take_break(self, short: bool = True):\n",
    "        \"\"\"Toma un descanso simulado.\"\"\"\n",
    "        if short:\n",
    "            duration = random.uniform(30, 120)\n",
    "            print(f\"‚òï Break corto: {duration:.0f}s\")\n",
    "        else:\n",
    "            duration = random.uniform(120, 600)\n",
    "            print(f\"üõãÔ∏è Break largo: {duration:.0f}s\")\n",
    "        time.sleep(duration)\n",
    "\n",
    "\n",
    "# Demostraci√≥n\n",
    "print(\"=\"*70)\n",
    "print(\"SIMULADOR DE COMPORTAMIENTO HUMANO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "simulator = HumanBehaviorSimulator(min_delay=0.5, max_delay=2.0)\n",
    "\n",
    "print(\"\\nSimulando 5 delays humanos:\")\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    simulator.wait_human_like()\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"   Delay {i+1}: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ROTADOR DE USER-AGENTS\n",
      "======================================================================\n",
      "\n",
      "5 User-Agents aleatorios:\n",
      "   1. Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (...\n",
      "   2. Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KH...\n",
      "   3. Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, l...\n",
      "   4. Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (...\n",
      "   5. Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KH...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4.2 ROTADOR DE USER-AGENTS\n",
    "# ==============================================================================\n",
    "\n",
    "class UserAgentRotator:\n",
    "    \"\"\"\n",
    "    Sistema de rotaci√≥n de User-Agents realistas.\n",
    "    \n",
    "    Un User-Agent como 'python-requests/2.28.0' grita 'SOY UN BOT'.\n",
    "    Esta clase proporciona User-Agents de navegadores reales.\n",
    "    \"\"\"\n",
    "    \n",
    "    # User-Agents actualizados (2024)\n",
    "    USER_AGENTS = {\n",
    "        'chrome_windows': [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "        ],\n",
    "        'chrome_mac': [\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        ],\n",
    "        'firefox_windows': [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n",
    "        ],\n",
    "        'safari_mac': [\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15\",\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    # Distribuci√≥n realista (Chrome domina)\n",
    "    WEIGHTS = {\n",
    "        'chrome_windows': 50,\n",
    "        'chrome_mac': 20,\n",
    "        'firefox_windows': 15,\n",
    "        'safari_mac': 15,\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._current_ua = None\n",
    "    \n",
    "    def get_random(self, force_new: bool = False) -> str:\n",
    "        \"\"\"Obtiene un User-Agent aleatorio.\"\"\"\n",
    "        if self._current_ua and not force_new:\n",
    "            return self._current_ua\n",
    "        \n",
    "        # Seleccionar categor√≠a seg√∫n pesos\n",
    "        categories = list(self.WEIGHTS.keys())\n",
    "        weights = list(self.WEIGHTS.values())\n",
    "        browser_type = random.choices(categories, weights=weights)[0]\n",
    "        \n",
    "        # Seleccionar UA de la categor√≠a\n",
    "        self._current_ua = random.choice(self.USER_AGENTS[browser_type])\n",
    "        return self._current_ua\n",
    "    \n",
    "    def get_full_headers(self) -> Dict[str, str]:\n",
    "        \"\"\"Obtiene headers completos consistentes con el UA.\"\"\"\n",
    "        ua = self.get_random()\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': ua,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'es-MX,es;q=0.9,en;q=0.8',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'DNT': '1',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "        \n",
    "        # A√±adir headers espec√≠ficos de Chrome\n",
    "        if 'Chrome' in ua:\n",
    "            headers['sec-ch-ua'] = '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\"'\n",
    "            headers['sec-ch-ua-mobile'] = '?0'\n",
    "            headers['sec-ch-ua-platform'] = '\"Windows\"'\n",
    "        \n",
    "        return headers\n",
    "    \n",
    "    def rotate(self) -> str:\n",
    "        \"\"\"Fuerza rotaci√≥n a nuevo UA.\"\"\"\n",
    "        return self.get_random(force_new=True)\n",
    "\n",
    "\n",
    "# Demostraci√≥n\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROTADOR DE USER-AGENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rotator = UserAgentRotator()\n",
    "\n",
    "print(\"\\n5 User-Agents aleatorios:\")\n",
    "for i in range(5):\n",
    "    ua = rotator.rotate()\n",
    "    print(f\"   {i+1}. {ua[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4.3 ROTADOR DE PROXIES\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ProxyInfo:\n",
    "    \"\"\"Informaci√≥n de un proxy.\"\"\"\n",
    "    url: str\n",
    "    failures: int = 0\n",
    "    success_count: int = 0\n",
    "    last_used: float = 0\n",
    "    \n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        total = self.success_count + self.failures\n",
    "        return self.success_count / total if total > 0 else 0.5\n",
    "    \n",
    "    def record_success(self):\n",
    "        self.success_count += 1\n",
    "        self.failures = 0\n",
    "        self.last_used = time.time()\n",
    "    \n",
    "    def record_failure(self):\n",
    "        self.failures += 1\n",
    "        self.last_used = time.time()\n",
    "\n",
    "\n",
    "class ProxyRotator:\n",
    "    \"\"\"\n",
    "    Sistema de rotaci√≥n de proxies.\n",
    "    \n",
    "    Tipos de proxies:\n",
    "    - Datacenter: R√°pidos y baratos, f√°ciles de detectar\n",
    "    - Residential: IPs de usuarios reales, dif√≠ciles de detectar\n",
    "    - Mobile: IPs de redes m√≥viles, casi imposibles de bloquear\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_failures: int = 3, cooldown: int = 300):\n",
    "        self.max_failures = max_failures\n",
    "        self.cooldown = cooldown\n",
    "        self.proxies: List[ProxyInfo] = []\n",
    "        self._index = 0\n",
    "    \n",
    "    def add_proxy(self, proxy_url: str):\n",
    "        \"\"\"A√±ade un proxy al pool.\"\"\"\n",
    "        self.proxies.append(ProxyInfo(url=proxy_url))\n",
    "    \n",
    "    def get_next(self) -> Optional[ProxyInfo]:\n",
    "        \"\"\"Obtiene el siguiente proxy disponible (round-robin).\"\"\"\n",
    "        if not self.proxies:\n",
    "            return None\n",
    "        \n",
    "        now = time.time()\n",
    "        available = [\n",
    "            p for p in self.proxies\n",
    "            if p.failures < self.max_failures or (now - p.last_used) > self.cooldown\n",
    "        ]\n",
    "        \n",
    "        if not available:\n",
    "            return None\n",
    "        \n",
    "        self._index = (self._index + 1) % len(available)\n",
    "        return available[self._index]\n",
    "    \n",
    "    def get_dict(self, proxy: Optional[ProxyInfo] = None) -> Dict[str, str]:\n",
    "        \"\"\"Obtiene dict para requests.\"\"\"\n",
    "        if proxy is None:\n",
    "            proxy = self.get_next()\n",
    "        if proxy is None:\n",
    "            return {}\n",
    "        return {'http': proxy.url, 'https': proxy.url}\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROTADOR DE PROXIES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nEjemplo de uso:\")\n",
    "print(\"\"\"\\n\n",
    "rotator = ProxyRotator()\n",
    "rotator.add_proxy(\"http://user:pass@proxy1.com:8080\")\n",
    "rotator.add_proxy(\"http://user:pass@proxy2.com:8080\")\n",
    "\n",
    "proxy = rotator.get_next()\n",
    "response = requests.get(url, proxies=rotator.get_dict(proxy))\n",
    "\n",
    "if response.ok:\n",
    "    proxy.record_success()\n",
    "else:\n",
    "    proxy.record_failure()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4.4 STEALTH SCRAPER - TODO INTEGRADO\n",
    "# ==============================================================================\n",
    "\n",
    "class StealthScraper:\n",
    "    \"\"\"\n",
    "    Scraper 'stealth' que integra todas las t√©cnicas anti-detecci√≥n.\n",
    "    \n",
    "    Combina:\n",
    "    - Rotaci√≥n de User-Agents\n",
    "    - Simulaci√≥n de comportamiento humano\n",
    "    - Headers realistas\n",
    "    - Detecci√≥n de bloqueos\n",
    "    \"\"\"\n",
    "    \n",
    "    BLOCK_INDICATORS = [\n",
    "        'captcha', 'blocked', 'access denied', 'rate limit',\n",
    "        'too many requests', 'suspicious activity'\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        delay_range: Tuple[float, float] = (1.0, 3.0),\n",
    "        max_retries: int = 3,\n",
    "    ):\n",
    "        self.ua_rotator = UserAgentRotator()\n",
    "        self.behavior = HumanBehaviorSimulator(\n",
    "            min_delay=delay_range[0],\n",
    "            max_delay=delay_range[1]\n",
    "        )\n",
    "        self.max_retries = max_retries\n",
    "        self.session = requests.Session()\n",
    "        self._blocked_count = 0\n",
    "        self._success_count = 0\n",
    "    \n",
    "    def _is_blocked(self, response: requests.Response) -> bool:\n",
    "        \"\"\"Detecta si fuimos bloqueados.\"\"\"\n",
    "        if response.status_code in [403, 429, 503]:\n",
    "            return True\n",
    "        content_lower = response.text.lower()\n",
    "        return any(ind in content_lower for ind in self.BLOCK_INDICATORS)\n",
    "    \n",
    "    def _handle_block(self):\n",
    "        \"\"\"Maneja detecci√≥n de bloqueo.\"\"\"\n",
    "        self._blocked_count += 1\n",
    "        print(f\"‚ö†Ô∏è Bloqueo detectado ({self._blocked_count} total)\")\n",
    "        \n",
    "        # Rotar UA\n",
    "        if self._blocked_count >= 3:\n",
    "            self.ua_rotator.rotate()\n",
    "            print(\"   Rotando User-Agent...\")\n",
    "        \n",
    "        # Break proporcional al n√∫mero de bloqueos\n",
    "        break_time = min(30 + (self._blocked_count * 10), 120)\n",
    "        print(f\"   Break de {break_time}s...\")\n",
    "        time.sleep(break_time)\n",
    "    \n",
    "    def get(self, url: str) -> Optional[requests.Response]:\n",
    "        \"\"\"\n",
    "        Realiza GET con todas las protecciones.\n",
    "        \n",
    "        Args:\n",
    "            url: URL a descargar\n",
    "            \n",
    "        Returns:\n",
    "            Response o None si falla\n",
    "        \"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                # Comportamiento humano\n",
    "                self.behavior.wait_human_like()\n",
    "                \n",
    "                # Headers realistas\n",
    "                headers = self.ua_rotator.get_full_headers()\n",
    "                \n",
    "                # Petici√≥n\n",
    "                response = self.session.get(url, headers=headers, timeout=30)\n",
    "                \n",
    "                # ¬øBloqueados?\n",
    "                if self._is_blocked(response):\n",
    "                    self._handle_block()\n",
    "                    continue\n",
    "                \n",
    "                # √âxito!\n",
    "                self._success_count += 1\n",
    "                self._blocked_count = max(0, self._blocked_count - 1)\n",
    "                return response\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Obtiene estad√≠sticas.\"\"\"\n",
    "        total = self._success_count + self._blocked_count\n",
    "        rate = self._success_count / total if total > 0 else 0\n",
    "        return {\n",
    "            'success': self._success_count,\n",
    "            'blocked': self._blocked_count,\n",
    "            'success_rate': f\"{rate:.1%}\"\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEALTH SCRAPER - TODAS LAS T√âCNICAS INTEGRADAS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Clase StealthScraper definida\")\n",
    "print(\"\\nCaracter√≠sticas:\")\n",
    "print(\"   ‚Ä¢ Rotaci√≥n de User-Agents\")\n",
    "print(\"   ‚Ä¢ Simulaci√≥n de comportamiento humano\")\n",
    "print(\"   ‚Ä¢ Headers realistas\")\n",
    "print(\"   ‚Ä¢ Detecci√≥n y manejo de bloqueos\")\n",
    "print(\"   ‚Ä¢ Reintentos con backoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PARTE 5: SELENIUM Y AUTOMATIZACI√ìN DE NAVEGADOR\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "## 5.1 ¬øCu√°ndo usar Selenium?\n",
    "\n",
    "| Situaci√≥n | Usar requests | Usar Selenium |\n",
    "|-----------|--------------|---------------|\n",
    "| Contenido en HTML inicial | ‚úÖ | ‚ùå |\n",
    "| Contenido cargado con JS | ‚ùå | ‚úÖ |\n",
    "| Login simple | ‚úÖ | ‚ùå |\n",
    "| Login con 2FA/OAuth | ‚ùå | ‚úÖ |\n",
    "| Miles de p√°ginas | ‚úÖ (r√°pido) | ‚ùå (lento) |\n",
    "| Captchas/anti-bot | ‚ùå | ‚úÖ |\n",
    "| Interacci√≥n compleja | ‚ùå | ‚úÖ |\n",
    "\n",
    "**Regla general**: Usa requests cuando sea posible, Selenium cuando sea necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selenium est√° disponible\n",
      "\n",
      "Para usar Selenium necesitas:\n",
      "   1. pip install selenium webdriver-manager\n",
      "   2. Chrome o Firefox instalado\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5.1 VERIFICAR DISPONIBILIDAD DE SELENIUM\n",
    "# ==============================================================================\n",
    "\n",
    "if SELENIUM_AVAILABLE:\n",
    "    print(\"‚úÖ Selenium est√° disponible\")\n",
    "    print(\"\\nPara usar Selenium necesitas:\")\n",
    "    print(\"   1. pip install selenium webdriver-manager\")\n",
    "    print(\"   2. Chrome o Firefox instalado\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Selenium no est√° instalado\")\n",
    "    print(\"\\nPara instalar:\")\n",
    "    print(\"   pip install selenium webdriver-manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n create_chrome_driver definida\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5.2 CONFIGURACI√ìN DE SELENIUM\n",
    "# ==============================================================================\n",
    "\n",
    "# Este c√≥digo solo se ejecuta si Selenium est√° disponible\n",
    "if SELENIUM_AVAILABLE:\n",
    "    \n",
    "    def create_chrome_driver(headless: bool = True) -> webdriver.Chrome:\n",
    "        \"\"\"\n",
    "        Crea un WebDriver de Chrome configurado para scraping.\n",
    "        \n",
    "        Args:\n",
    "            headless: Si ejecutar sin interfaz gr√°fica\n",
    "            \n",
    "        Returns:\n",
    "            WebDriver configurado\n",
    "        \"\"\"\n",
    "        options = Options()\n",
    "        \n",
    "        # Modo headless (sin ventana)\n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        \n",
    "        # Argumentos de rendimiento\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        \n",
    "        # Anti-detecci√≥n b√°sica\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "        \n",
    "        # Tama√±o de ventana\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        \n",
    "        # Crear driver\n",
    "        try:\n",
    "            from webdriver_manager.chrome import ChromeDriverManager\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            driver = webdriver.Chrome(service=service, options=options)\n",
    "        except:\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "        # Ocultar webdriver\n",
    "        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "            'source': \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n",
    "        })\n",
    "        \n",
    "        return driver\n",
    "    \n",
    "    print(\"‚úÖ Funci√≥n create_chrome_driver definida\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Selenium no disponible - saltando configuraci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clase SeleniumScraper definida\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5.3 CLASE SELENIUM SCRAPER\n",
    "# ==============================================================================\n",
    "\n",
    "if SELENIUM_AVAILABLE:\n",
    "    \n",
    "    class SeleniumScraper:\n",
    "        \"\"\"\n",
    "        Scraper completo usando Selenium.\n",
    "        \n",
    "        Integra:\n",
    "        - Configuraci√≥n anti-detecci√≥n\n",
    "        - Localizaci√≥n de elementos\n",
    "        - Esperas inteligentes\n",
    "        - Interacci√≥n con la p√°gina\n",
    "        \n",
    "        Example:\n",
    "            >>> with SeleniumScraper(headless=True) as scraper:\n",
    "            ...     scraper.get(\"https://ejemplo.com\")\n",
    "            ...     titulo = scraper.find(\".title\").text\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, headless: bool = True, timeout: int = 30):\n",
    "            self.timeout = timeout\n",
    "            self.driver = create_chrome_driver(headless=headless)\n",
    "            self.driver.implicitly_wait(10)\n",
    "        \n",
    "        def get(self, url: str, wait_for_load: bool = True):\n",
    "            \"\"\"Navega a una URL.\"\"\"\n",
    "            self.driver.get(url)\n",
    "            if wait_for_load:\n",
    "                WebDriverWait(self.driver, self.timeout).until(\n",
    "                    lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "                )\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "        \n",
    "        def find(self, selector: str):\n",
    "            \"\"\"Encuentra un elemento por CSS selector.\"\"\"\n",
    "            try:\n",
    "                return WebDriverWait(self.driver, self.timeout).until(\n",
    "                    EC.visibility_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        def find_all(self, selector: str):\n",
    "            \"\"\"Encuentra todos los elementos.\"\"\"\n",
    "            return self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "        \n",
    "        def click(self, selector: str):\n",
    "            \"\"\"Hace click en un elemento.\"\"\"\n",
    "            elem = WebDriverWait(self.driver, self.timeout).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n",
    "            )\n",
    "            elem.click()\n",
    "            time.sleep(random.uniform(0.3, 0.8))\n",
    "        \n",
    "        def type_text(self, selector: str, text: str, human_like: bool = True):\n",
    "            \"\"\"Escribe texto en un input.\"\"\"\n",
    "            elem = self.find(selector)\n",
    "            elem.clear()\n",
    "            \n",
    "            if human_like:\n",
    "                for char in text:\n",
    "                    elem.send_keys(char)\n",
    "                    time.sleep(random.uniform(0.05, 0.15))\n",
    "            else:\n",
    "                elem.send_keys(text)\n",
    "        \n",
    "        def scroll_to_bottom(self):\n",
    "            \"\"\"Hace scroll hasta el final.\"\"\"\n",
    "            self.driver.execute_script(\n",
    "                \"window.scrollTo({top: document.body.scrollHeight, behavior: 'smooth'});\"\n",
    "            )\n",
    "            time.sleep(1)\n",
    "        \n",
    "        def get_page_source(self) -> str:\n",
    "            \"\"\"Obtiene el HTML actual.\"\"\"\n",
    "            return self.driver.page_source\n",
    "        \n",
    "        def screenshot(self, filename: str):\n",
    "            \"\"\"Toma screenshot.\"\"\"\n",
    "            self.driver.save_screenshot(filename)\n",
    "        \n",
    "        def close(self):\n",
    "            \"\"\"Cierra el navegador.\"\"\"\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        \n",
    "        def __exit__(self, *args):\n",
    "            self.close()\n",
    "    \n",
    "    print(\"‚úÖ Clase SeleniumScraper definida\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Selenium no disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 XPath y CSS Selectors en Selenium\n",
    "\n",
    "### CSS Selectors\n",
    "```python\n",
    "# Por ID\n",
    "driver.find_element(By.CSS_SELECTOR, \"#login-button\")\n",
    "\n",
    "# Por clase\n",
    "driver.find_element(By.CSS_SELECTOR, \".product-item\")\n",
    "\n",
    "# Por atributo\n",
    "driver.find_element(By.CSS_SELECTOR, \"input[name='email']\")\n",
    "\n",
    "# Combinado\n",
    "driver.find_element(By.CSS_SELECTOR, \"div.container > form#login input.email\")\n",
    "```\n",
    "\n",
    "### XPath\n",
    "```python\n",
    "# Por texto\n",
    "driver.find_element(By.XPATH, \"//button[text()='Enviar']\")\n",
    "\n",
    "# Contiene texto\n",
    "driver.find_element(By.XPATH, \"//a[contains(text(), 'Ver m√°s')]\")\n",
    "\n",
    "# Por atributo\n",
    "driver.find_element(By.XPATH, \"//input[@placeholder='Email']\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PARTE 6: PROYECTO FINAL INTEGRADO\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "## 6.1 Arquitectura de un Scraper de Producci√≥n\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    ARQUITECTURA                                 ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ   CONFIG ‚îÄ‚îÄ‚ñ∂ SCHEDULER ‚îÄ‚îÄ‚ñ∂ DOWNLOADER ‚îÄ‚îÄ‚ñ∂ PARSER               ‚îÇ\n",
    "‚îÇ                ‚îÇ               ‚îÇ            ‚îÇ                   ‚îÇ\n",
    "‚îÇ                ‚ñº               ‚ñº            ‚ñº                   ‚îÇ\n",
    "‚îÇ            FRONTIER       STORAGE      PIPELINE                 ‚îÇ\n",
    "‚îÇ                                            ‚îÇ                   ‚îÇ\n",
    "‚îÇ                                            ‚ñº                   ‚îÇ\n",
    "‚îÇ                                        EXPORTER                 ‚îÇ\n",
    "‚îÇ                                       (CSV/JSON)                ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Estructuras de datos de producci√≥n definidas\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 6.1 ESTRUCTURAS DE DATOS PARA PRODUCCI√ìN\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ScrapedItem:\n",
    "    \"\"\"\n",
    "    Item de datos extra√≠do de una p√°gina.\n",
    "    \n",
    "    Estructura gen√©rica para almacenar cualquier dato scrapeado.\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    data: Dict[str, Any]\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "    page_title: Optional[str] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convierte a diccionario plano.\"\"\"\n",
    "        result = {\n",
    "            'url': self.url,\n",
    "            'timestamp': self.timestamp.isoformat(),\n",
    "            'page_title': self.page_title,\n",
    "        }\n",
    "        result.update(self.data)\n",
    "        return result\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScraperConfig:\n",
    "    \"\"\"Configuraci√≥n centralizada del scraper.\"\"\"\n",
    "    name: str = \"MyScraper\"\n",
    "    seed_urls: List[str] = field(default_factory=list)\n",
    "    max_pages: int = 100\n",
    "    max_depth: int = 5\n",
    "    min_delay: float = 1.0\n",
    "    max_delay: float = 3.0\n",
    "    output_dir: str = \"./output\"\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "\n",
    "print(\"‚úÖ Estructuras de datos de producci√≥n definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exportadores CSV y JSON definidos\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 6.2 EXPORTADORES DE DATOS\n",
    "# ==============================================================================\n",
    "\n",
    "class CSVExporter:\n",
    "    \"\"\"Exporta datos a CSV.\"\"\"\n",
    "    \n",
    "    def __init__(self, filepath: str):\n",
    "        self.filepath = filepath\n",
    "        self._header_written = False\n",
    "        os.makedirs(os.path.dirname(filepath) or '.', exist_ok=True)\n",
    "    \n",
    "    def export_one(self, item: ScrapedItem):\n",
    "        \"\"\"Exporta un item (append).\"\"\"\n",
    "        data = item.to_dict()\n",
    "        mode = 'a' if self._header_written else 'w'\n",
    "        \n",
    "        with open(self.filepath, mode, newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data.keys())\n",
    "            if not self._header_written:\n",
    "                writer.writeheader()\n",
    "                self._header_written = True\n",
    "            writer.writerow(data)\n",
    "    \n",
    "    def export_all(self, items: List[ScrapedItem]):\n",
    "        \"\"\"Exporta todos los items.\"\"\"\n",
    "        for item in items:\n",
    "            self.export_one(item)\n",
    "        print(f\"‚úÖ Exportados {len(items)} items a {self.filepath}\")\n",
    "\n",
    "\n",
    "class JSONExporter:\n",
    "    \"\"\"Exporta datos a JSON.\"\"\"\n",
    "    \n",
    "    def __init__(self, filepath: str):\n",
    "        self.filepath = filepath\n",
    "        self._items: List[Dict] = []\n",
    "        os.makedirs(os.path.dirname(filepath) or '.', exist_ok=True)\n",
    "    \n",
    "    def export_one(self, item: ScrapedItem):\n",
    "        \"\"\"A√±ade item a lista interna.\"\"\"\n",
    "        self._items.append(item.to_dict())\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"Escribe todos los items al archivo.\"\"\"\n",
    "        with open(self.filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self._items, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Exportados {len(self._items)} items a {self.filepath}\")\n",
    "\n",
    "print(\"‚úÖ Exportadores CSV y JSON definidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clase ProductionScraper definida\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 6.3 PRODUCTION SCRAPER COMPLETO\n",
    "# ==============================================================================\n",
    "\n",
    "class ProductionScraper:\n",
    "    \"\"\"\n",
    "    Scraper de producci√≥n que integra todos los componentes.\n",
    "    \n",
    "    Caracter√≠sticas:\n",
    "    - Crawling BFS configurable\n",
    "    - Descarga robusta con reintentos\n",
    "    - Extracci√≥n de datos\n",
    "    - Exportaci√≥n a CSV/JSON\n",
    "    - Logging completo\n",
    "    - Deduplicaci√≥n\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ScraperConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        # Estado\n",
    "        self.frontier: deque = deque()\n",
    "        self.visited: Set[str] = set()\n",
    "        self.all_items: List[ScrapedItem] = []\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        self.stats = {\n",
    "            'pages_crawled': 0,\n",
    "            'pages_success': 0,\n",
    "            'items_extracted': 0,\n",
    "            'start_time': None,\n",
    "        }\n",
    "        \n",
    "        # Sesi√≥n HTTP\n",
    "        self.session = requests.Session()\n",
    "        self.ua_rotator = UserAgentRotator()\n",
    "        self.behavior = HumanBehaviorSimulator(\n",
    "            min_delay=config.min_delay,\n",
    "            max_delay=config.max_delay\n",
    "        )\n",
    "        \n",
    "        # Exportadores\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "        self.csv_exporter = CSVExporter(f\"{config.output_dir}/data.csv\")\n",
    "        self.json_exporter = JSONExporter(f\"{config.output_dir}/data.json\")\n",
    "        \n",
    "        # Extraer dominios permitidos\n",
    "        self.allowed_domains = {urlparse(u).netloc for u in config.seed_urls}\n",
    "        \n",
    "        logger.info(f\"Scraper '{config.name}' inicializado\")\n",
    "    \n",
    "    def _normalize_url(self, url: str) -> str:\n",
    "        parsed = urlparse(url)\n",
    "        path = parsed.path.rstrip('/') if parsed.path != '/' else '/'\n",
    "        return urlunparse((parsed.scheme, parsed.netloc.lower(), path, '', parsed.query, ''))\n",
    "    \n",
    "    def _is_valid_url(self, url: str, depth: int) -> bool:\n",
    "        if url in self.visited:\n",
    "            return False\n",
    "        if depth > self.config.max_depth:\n",
    "            return False\n",
    "        \n",
    "        parsed = urlparse(url)\n",
    "        if parsed.scheme not in ('http', 'https'):\n",
    "            return False\n",
    "        if parsed.netloc not in self.allowed_domains:\n",
    "            return False\n",
    "        \n",
    "        blocked_ext = {'.jpg', '.png', '.pdf', '.zip', '.css', '.js'}\n",
    "        if any(parsed.path.lower().endswith(ext) for ext in blocked_ext):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _download(self, url: str) -> Tuple[Optional[str], int]:\n",
    "        try:\n",
    "            headers = self.ua_rotator.get_full_headers()\n",
    "            response = self.session.get(url, headers=headers, timeout=30)\n",
    "            return response.text, response.status_code\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error descargando {url}: {e}\")\n",
    "            return None, 0\n",
    "    \n",
    "    def _extract_links(self, html: str, base_url: str) -> List[str]:\n",
    "        links = []\n",
    "        try:\n",
    "            tree = lxml_html.fromstring(html)\n",
    "            for href in tree.xpath('//a/@href'):\n",
    "                if href and not href.startswith(('#', 'javascript:', 'mailto:')):\n",
    "                    absolute = urljoin(base_url, href)\n",
    "                    normalized = self._normalize_url(absolute)\n",
    "                    if normalized not in links:\n",
    "                        links.append(normalized)\n",
    "        except:\n",
    "            pass\n",
    "        return links\n",
    "    \n",
    "    def _extract_data(self, html: str, url: str) -> List[ScrapedItem]:\n",
    "        \"\"\"\n",
    "        Extrae datos de la p√°gina.\n",
    "        Sobrescribir este m√©todo para extracci√≥n personalizada.\n",
    "        \"\"\"\n",
    "        items = []\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            \n",
    "            # Extraer metadatos b√°sicos\n",
    "            title = soup.find('title')\n",
    "            title_text = title.get_text(strip=True) if title else None\n",
    "            \n",
    "            data = {\n",
    "                'num_links': len(soup.find_all('a')),\n",
    "                'num_images': len(soup.find_all('img')),\n",
    "            }\n",
    "            \n",
    "            item = ScrapedItem(url=url, data=data, page_title=title_text)\n",
    "            items.append(item)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extrayendo datos: {e}\")\n",
    "        \n",
    "        return items\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Ejecuta el scraping.\"\"\"\n",
    "        self.stats['start_time'] = datetime.now()\n",
    "        \n",
    "        # Inicializar frontier\n",
    "        for url in self.config.seed_urls:\n",
    "            normalized = self._normalize_url(url)\n",
    "            self.frontier.append((normalized, 0))\n",
    "            self.visited.add(normalized)\n",
    "        \n",
    "        logger.info(f\"Iniciando scraping con {len(self.config.seed_urls)} semillas\")\n",
    "        \n",
    "        # Loop principal\n",
    "        with tqdm(total=self.config.max_pages, desc=\"Scraping\") as pbar:\n",
    "            while self.frontier and self.stats['pages_crawled'] < self.config.max_pages:\n",
    "                # BFS: popleft\n",
    "                url, depth = self.frontier.popleft()\n",
    "                \n",
    "                # Comportamiento humano\n",
    "                self.behavior.wait_human_like()\n",
    "                \n",
    "                # Descargar\n",
    "                content, status = self._download(url)\n",
    "                \n",
    "                self.stats['pages_crawled'] += 1\n",
    "                \n",
    "                if content and 200 <= status < 300:\n",
    "                    self.stats['pages_success'] += 1\n",
    "                    \n",
    "                    # Extraer datos\n",
    "                    items = self._extract_data(content, url)\n",
    "                    for item in items:\n",
    "                        self.all_items.append(item)\n",
    "                        self.csv_exporter.export_one(item)\n",
    "                        self.json_exporter.export_one(item)\n",
    "                        self.stats['items_extracted'] += 1\n",
    "                    \n",
    "                    # Extraer links\n",
    "                    links = self._extract_links(content, url)\n",
    "                    for link in links:\n",
    "                        if self._is_valid_url(link, depth + 1):\n",
    "                            self.frontier.append((link, depth + 1))\n",
    "                            self.visited.add(link)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'items': self.stats['items_extracted']})\n",
    "        \n",
    "        # Finalizar\n",
    "        self.json_exporter.finalize()\n",
    "        \n",
    "        # Resumen\n",
    "        duration = (datetime.now() - self.stats['start_time']).total_seconds()\n",
    "        logger.info(\"=\"*50)\n",
    "        logger.info(\"SCRAPING COMPLETADO\")\n",
    "        logger.info(f\"P√°ginas: {self.stats['pages_crawled']}\")\n",
    "        logger.info(f\"Items: {self.stats['items_extracted']}\")\n",
    "        logger.info(f\"Duraci√≥n: {duration:.1f}s\")\n",
    "        logger.info(\"=\"*50)\n",
    "        \n",
    "        return self.stats\n",
    "\n",
    "print(\"‚úÖ Clase ProductionScraper definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# PARTE 7: AN√ÅLISIS DE DATOS SCRAPEADOS (CSV INTEGRATION)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "## 7.1 Cargando y Analizando Datos Scrapeados\n",
    "\n",
    "En esta secci√≥n analizamos datos reales obtenidos mediante web scraping.\n",
    "Usaremos el dataset `wikipedia_scrape_results_cleaned.csv` que contiene\n",
    "informaci√≥n de empresas scrapeada de Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATOS SCRAPEADOS DE WIKIPEDIA\n",
      "======================================================================\n",
      "\n",
      "üìä Dimensiones del dataset: (335, 3)\n",
      "\n",
      "üìã Columnas: ['Company Name', 'Revenue', 'Profit']\n",
      "\n",
      "üîç Primeras 10 filas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>Retail</td>\n",
       "      <td>$648,125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Retail\\ninformation technology</td>\n",
       "      <td>$574,785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>State Grid Corporation of China</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>$545,948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Saudi Aramco</td>\n",
       "      <td>Oil and gas</td>\n",
       "      <td>$488,980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>China Petrochemical Corporation</td>\n",
       "      <td>Oil and gas</td>\n",
       "      <td>$429,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>China National Petroleum Corporation</td>\n",
       "      <td>Oil and gas</td>\n",
       "      <td>$476,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vitol</td>\n",
       "      <td>Commodities</td>\n",
       "      <td>$400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Information technology</td>\n",
       "      <td>$383,285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Company Name                         Revenue  \\\n",
       "0                                   NaN                             NaN   \n",
       "1                                   NaN                             NaN   \n",
       "2                               Walmart                          Retail   \n",
       "3                                Amazon  Retail\\ninformation technology   \n",
       "4       State Grid Corporation of China                     Electricity   \n",
       "5                          Saudi Aramco                     Oil and gas   \n",
       "6       China Petrochemical Corporation                     Oil and gas   \n",
       "7  China National Petroleum Corporation                     Oil and gas   \n",
       "8                                 Vitol                     Commodities   \n",
       "9                                 Apple          Information technology   \n",
       "\n",
       "     Profit  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2  $648,125  \n",
       "3  $574,785  \n",
       "4  $545,948  \n",
       "5  $488,980  \n",
       "6  $429,700  \n",
       "7  $476,000  \n",
       "8  $400,000  \n",
       "9  $383,285  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 7.1 CARGAR DATOS CSV SCRAPEADOS\n",
    "# ==============================================================================\n",
    "\n",
    "# Cargar el archivo CSV con datos de Wikipedia\n",
    "# Este archivo fue generado mediante web scraping de la lista de\n",
    "# las empresas m√°s grandes del mundo por ingresos\n",
    "\n",
    "try:\n",
    "    # Intentar cargar desde la ubicaci√≥n del usuario\n",
    "    df_wikipedia = pd.read_csv('wikipedia_scrape_results_cleaned.csv')\n",
    "except FileNotFoundError:\n",
    "    # Si no existe, crear datos de ejemplo\n",
    "    print(\"‚ö†Ô∏è Archivo no encontrado, creando datos de ejemplo...\")\n",
    "    df_wikipedia = pd.DataFrame({\n",
    "        'Company Name': ['Walmart', 'Amazon', 'Apple', 'Microsoft', 'Alphabet'],\n",
    "        'Revenue': ['Retail', 'Retail\\ninformation technology', 'Information technology', 'Information technology', 'Information technology'],\n",
    "        'Profit': ['$648,125', '$574,785', '$383,285', '$245,122', '$307,394']\n",
    "    })\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATOS SCRAPEADOS DE WIKIPEDIA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Dimensiones del dataset: {df_wikipedia.shape}\")\n",
    "print(f\"\\nüìã Columnas: {list(df_wikipedia.columns)}\")\n",
    "print(f\"\\nüîç Primeras 10 filas:\")\n",
    "df_wikipedia.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATOS DESPU√âS DE LIMPIEZA\n",
      "======================================================================\n",
      "\n",
      "üìä Dimensiones: (320, 4)\n",
      "\n",
      "üìã Columnas: ['empresa', 'sector', 'ingresos_usd', 'ingresos_num']\n",
      "\n",
      "üîç Muestra de datos limpios:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>empresa</th>\n",
       "      <th>sector</th>\n",
       "      <th>ingresos_usd</th>\n",
       "      <th>ingresos_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>Retail</td>\n",
       "      <td>648125</td>\n",
       "      <td>648125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Retail / information technology</td>\n",
       "      <td>574785</td>\n",
       "      <td>574785.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>State Grid Corporation of China</td>\n",
       "      <td>Electricity</td>\n",
       "      <td>545948</td>\n",
       "      <td>545948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Saudi Aramco</td>\n",
       "      <td>Oil and gas</td>\n",
       "      <td>488980</td>\n",
       "      <td>488980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>China Petrochemical Corporation</td>\n",
       "      <td>Oil and gas</td>\n",
       "      <td>429700</td>\n",
       "      <td>429700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>China National Petroleum Corporation</td>\n",
       "      <td>Oil and gas</td>\n",
       "      <td>476000</td>\n",
       "      <td>476000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vitol</td>\n",
       "      <td>Commodities</td>\n",
       "      <td>400000</td>\n",
       "      <td>400000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Information technology</td>\n",
       "      <td>383285</td>\n",
       "      <td>383285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>371622</td>\n",
       "      <td>371622.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Berkshire Hathaway</td>\n",
       "      <td>Financials</td>\n",
       "      <td>364482</td>\n",
       "      <td>364482.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 empresa                           sector  \\\n",
       "2                                Walmart                           Retail   \n",
       "3                                 Amazon  Retail / information technology   \n",
       "4        State Grid Corporation of China                      Electricity   \n",
       "5                           Saudi Aramco                      Oil and gas   \n",
       "6        China Petrochemical Corporation                      Oil and gas   \n",
       "7   China National Petroleum Corporation                      Oil and gas   \n",
       "8                                  Vitol                      Commodities   \n",
       "9                                  Apple           Information technology   \n",
       "10                    UnitedHealth Group                       Healthcare   \n",
       "11                    Berkshire Hathaway                       Financials   \n",
       "\n",
       "   ingresos_usd  ingresos_num  \n",
       "2        648125      648125.0  \n",
       "3        574785      574785.0  \n",
       "4        545948      545948.0  \n",
       "5        488980      488980.0  \n",
       "6        429700      429700.0  \n",
       "7        476000      476000.0  \n",
       "8        400000      400000.0  \n",
       "9        383285      383285.0  \n",
       "10       371622      371622.0  \n",
       "11       364482      364482.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 7.2 LIMPIEZA DE DATOS SCRAPEADOS\n",
    "# ==============================================================================\n",
    "\n",
    "def limpiar_datos_scrapeados(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpia y procesa datos scrapeados.\n",
    "    \n",
    "    Los datos de web scraping suelen necesitar limpieza:\n",
    "    - Eliminar caracteres especiales\n",
    "    - Convertir tipos de datos\n",
    "    - Manejar valores faltantes\n",
    "    - Normalizar texto\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con datos crudos\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame limpio\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Eliminar filas completamente vac√≠as\n",
    "    df_clean = df_clean.dropna(how='all')\n",
    "    \n",
    "    # Renombrar columnas para consistencia\n",
    "    column_mapping = {\n",
    "        'Company Name': 'empresa',\n",
    "        'Revenue': 'sector',  # Nota: parece que los datos tienen las columnas mal nombradas\n",
    "        'Profit': 'ingresos_usd'\n",
    "    }\n",
    "    \n",
    "    # Aplicar mapeo solo a columnas existentes\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_clean.columns:\n",
    "            df_clean = df_clean.rename(columns={old_name: new_name})\n",
    "    \n",
    "    # Limpiar columna de ingresos (si existe)\n",
    "    if 'ingresos_usd' in df_clean.columns:\n",
    "        # Eliminar $, comas y convertir a num√©rico\n",
    "        df_clean['ingresos_usd'] = (\n",
    "            df_clean['ingresos_usd']\n",
    "            .astype(str)\n",
    "            .str.replace('$', '', regex=False)\n",
    "            .str.replace(',', '', regex=False)\n",
    "            .str.strip()\n",
    "        )\n",
    "        df_clean['ingresos_num'] = pd.to_numeric(df_clean['ingresos_usd'], errors='coerce')\n",
    "    \n",
    "    # Limpiar columna de sector (eliminar saltos de l√≠nea)\n",
    "    if 'sector' in df_clean.columns:\n",
    "        df_clean['sector'] = df_clean['sector'].astype(str).str.replace('\\n', ' / ', regex=False)\n",
    "    \n",
    "    # Eliminar filas sin nombre de empresa\n",
    "    if 'empresa' in df_clean.columns:\n",
    "        df_clean = df_clean[df_clean['empresa'].notna() & (df_clean['empresa'] != '')]\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Aplicar limpieza\n",
    "df_limpio = limpiar_datos_scrapeados(df_wikipedia)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATOS DESPU√âS DE LIMPIEZA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Dimensiones: {df_limpio.shape}\")\n",
    "print(f\"\\nüìã Columnas: {list(df_limpio.columns)}\")\n",
    "print(f\"\\nüîç Muestra de datos limpios:\")\n",
    "df_limpio.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AN√ÅLISIS EXPLORATORIO DE DATOS\n",
      "======================================================================\n",
      "\n",
      "üìä INFORMACI√ìN GENERAL\n",
      "--------------------------------------------------\n",
      "Total de empresas: 320\n",
      "Columnas: ['empresa', 'sector', 'ingresos_usd', 'ingresos_num']\n",
      "\n",
      "‚ö†Ô∏è VALORES FALTANTES\n",
      "--------------------------------------------------\n",
      "   empresa: 0 (0.0%)\n",
      "   sector: 0 (0.0%)\n",
      "   ingresos_usd: 70 (21.9%)\n",
      "   ingresos_num: 70 (21.9%)\n",
      "\n",
      "üí∞ ESTAD√çSTICAS DE INGRESOS (millones USD)\n",
      "--------------------------------------------------\n",
      "   Media:    $270,534\n",
      "   Mediana:  $218,387\n",
      "   M√≠nimo:   $147,100\n",
      "   M√°ximo:   $648,125\n",
      "   Desv Est: $120,025\n",
      "\n",
      "üèÜ TOP 10 EMPRESAS POR INGRESOS\n",
      "--------------------------------------------------\n",
      "    1. Walmart                   | $   648,125 | Retail\n",
      "    2. Walmart                   | $   648,125 | Retail\n",
      "    3. Walmart                   | $   648,125 | Retail\n",
      "    4. Walmart                   | $   648,125 | Retail\n",
      "    5. Walmart                   | $   648,125 | Retail\n",
      "    6. Amazon                    | $   574,785 | Retail / information\n",
      "    7. Amazon                    | $   574,785 | Retail / information\n",
      "    8. Amazon                    | $   574,785 | Retail / information\n",
      "    9. Amazon                    | $   574,785 | Retail / information\n",
      "   10. Amazon                    | $   574,785 | Retail / information\n",
      "\n",
      "üìà EMPRESAS POR SECTOR\n",
      "--------------------------------------------------\n",
      "   1                                   |  45 empresas\n",
      "   Oil and gas                         |  40 empresas\n",
      "   Financials                          |  40 empresas\n",
      "   Healthcare                          |  35 empresas\n",
      "   Automotive                          |  35 empresas\n",
      "   Retail                              |  15 empresas\n",
      "   Commodities                         |  15 empresas\n",
      "   Information technology              |  15 empresas\n",
      "   Construction                        |  15 empresas\n",
      "   Electronics                         |  10 empresas\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 7.3 AN√ÅLISIS EXPLORATORIO DE DATOS (EDA)\n",
    "# ==============================================================================\n",
    "\n",
    "def analisis_exploratorio(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Realiza an√°lisis exploratorio de datos scrapeados.\n",
    "    \n",
    "    Incluye:\n",
    "    - Estad√≠sticas descriptivas\n",
    "    - An√°lisis por sector\n",
    "    - Top empresas\n",
    "    - Calidad de datos\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"AN√ÅLISIS EXPLORATORIO DE DATOS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Informaci√≥n general\n",
    "    print(\"\\nüìä INFORMACI√ìN GENERAL\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Total de empresas: {len(df)}\")\n",
    "    print(f\"Columnas: {list(df.columns)}\")\n",
    "    \n",
    "    # Valores faltantes\n",
    "    print(\"\\n‚ö†Ô∏è VALORES FALTANTES\")\n",
    "    print(\"-\"*50)\n",
    "    missing = df.isnull().sum()\n",
    "    for col, count in missing.items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"   {col}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # An√°lisis de ingresos (si existe la columna num√©rica)\n",
    "    if 'ingresos_num' in df.columns:\n",
    "        print(\"\\nüí∞ ESTAD√çSTICAS DE INGRESOS (millones USD)\")\n",
    "        print(\"-\"*50)\n",
    "        stats = df['ingresos_num'].describe()\n",
    "        print(f\"   Media:    ${stats['mean']:,.0f}\")\n",
    "        print(f\"   Mediana:  ${stats['50%']:,.0f}\")\n",
    "        print(f\"   M√≠nimo:   ${stats['min']:,.0f}\")\n",
    "        print(f\"   M√°ximo:   ${stats['max']:,.0f}\")\n",
    "        print(f\"   Desv Est: ${stats['std']:,.0f}\")\n",
    "        \n",
    "        # Top 10 empresas\n",
    "        print(\"\\nüèÜ TOP 10 EMPRESAS POR INGRESOS\")\n",
    "        print(\"-\"*50)\n",
    "        top10 = df.nlargest(10, 'ingresos_num')[['empresa', 'sector', 'ingresos_num']]\n",
    "        for i, (_, row) in enumerate(top10.iterrows(), 1):\n",
    "            print(f\"   {i:2}. {row['empresa'][:25]:25} | ${row['ingresos_num']:>10,.0f} | {str(row['sector'])[:20]}\")\n",
    "    \n",
    "    # An√°lisis por sector\n",
    "    if 'sector' in df.columns:\n",
    "        print(\"\\nüìà EMPRESAS POR SECTOR\")\n",
    "        print(\"-\"*50)\n",
    "        sector_counts = df['sector'].value_counts().head(10)\n",
    "        for sector, count in sector_counts.items():\n",
    "            print(f\"   {str(sector)[:35]:35} | {count:3} empresas\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Ejecutar an√°lisis\n",
    "analisis_exploratorio(df_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:58:51 [INFO] Pipeline 'wikipedia_empresas' inicializado\n",
      "05:58:51 [INFO] Ejecutando pipeline 'wikipedia_empresas'\n",
      "05:58:51 [INFO] Datos limpiados: 320 registros\n",
      "05:58:51 [INFO] Exportado: ./output_pipeline/wikipedia_empresas_clean.csv\n",
      "05:58:51 [INFO] Exportado: ./output_pipeline/wikipedia_empresas_clean.json\n",
      "05:58:51 [INFO] Pipeline 'wikipedia_empresas' completado\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEMOSTRACI√ìN DEL PIPELINE COMPLETO\n",
      "======================================================================\n",
      "\n",
      "üìä M√©tricas del an√°lisis:\n",
      "   Total registros: 320\n",
      "   Columnas: ['empresa', 'sector', 'ingresos_usd', 'ingresos_num']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 7.5 PIPELINE COMPLETO: SCRAPING ‚Üí AN√ÅLISIS\n",
    "# ==============================================================================\n",
    "\n",
    "class ScrapingPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline completo que integra scraping y an√°lisis.\n",
    "    \n",
    "    Flujo:\n",
    "    1. Scraping de datos\n",
    "    2. Limpieza y transformaci√≥n\n",
    "    3. An√°lisis exploratorio\n",
    "    4. Exportaci√≥n de resultados\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, output_dir: str = './output'):\n",
    "        self.name = name\n",
    "        self.output_dir = output_dir\n",
    "        self.raw_data: List[Dict] = []\n",
    "        self.df_clean: Optional[pd.DataFrame] = None\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        logger.info(f\"Pipeline '{name}' inicializado\")\n",
    "    \n",
    "    def add_data(self, data: Dict):\n",
    "        \"\"\"A√±ade datos crudos al pipeline.\"\"\"\n",
    "        self.raw_data.append(data)\n",
    "    \n",
    "    def load_csv(self, filepath: str):\n",
    "        \"\"\"Carga datos desde CSV existente.\"\"\"\n",
    "        df = pd.read_csv(filepath)\n",
    "        self.raw_data = df.to_dict('records')\n",
    "        logger.info(f\"Cargados {len(self.raw_data)} registros de {filepath}\")\n",
    "    \n",
    "    def clean_data(self, cleaning_func: Optional[Callable] = None):\n",
    "        \"\"\"\n",
    "        Limpia los datos usando funci√≥n proporcionada.\n",
    "        \n",
    "        Args:\n",
    "            cleaning_func: Funci√≥n de limpieza personalizada\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self.raw_data)\n",
    "        \n",
    "        if cleaning_func:\n",
    "            self.df_clean = cleaning_func(df)\n",
    "        else:\n",
    "            # Limpieza b√°sica por defecto\n",
    "            self.df_clean = df.dropna(how='all')\n",
    "            for col in self.df_clean.select_dtypes(include=['object']).columns:\n",
    "                self.df_clean[col] = self.df_clean[col].str.strip()\n",
    "        \n",
    "        logger.info(f\"Datos limpiados: {len(self.df_clean)} registros\")\n",
    "    \n",
    "    def analyze(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Realiza an√°lisis y retorna m√©tricas.\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con m√©tricas del an√°lisis\n",
    "        \"\"\"\n",
    "        if self.df_clean is None:\n",
    "            raise ValueError(\"Primero ejecuta clean_data()\")\n",
    "        \n",
    "        metrics = {\n",
    "            'total_records': len(self.df_clean),\n",
    "            'columns': list(self.df_clean.columns),\n",
    "            'missing_values': self.df_clean.isnull().sum().to_dict(),\n",
    "            'dtypes': self.df_clean.dtypes.astype(str).to_dict(),\n",
    "        }\n",
    "        \n",
    "        # Estad√≠sticas num√©ricas\n",
    "        numeric_cols = self.df_clean.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            metrics['numeric_stats'] = self.df_clean[numeric_cols].describe().to_dict()\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def export(self, formats: List[str] = ['csv', 'json']):\n",
    "        \"\"\"\n",
    "        Exporta datos limpios en m√∫ltiples formatos.\n",
    "        \n",
    "        Args:\n",
    "            formats: Lista de formatos ('csv', 'json', 'excel')\n",
    "        \"\"\"\n",
    "        if self.df_clean is None:\n",
    "            raise ValueError(\"Primero ejecuta clean_data()\")\n",
    "        \n",
    "        base_path = f\"{self.output_dir}/{self.name}\"\n",
    "        \n",
    "        if 'csv' in formats:\n",
    "            path = f\"{base_path}_clean.csv\"\n",
    "            self.df_clean.to_csv(path, index=False)\n",
    "            logger.info(f\"Exportado: {path}\")\n",
    "        \n",
    "        if 'json' in formats:\n",
    "            path = f\"{base_path}_clean.json\"\n",
    "            self.df_clean.to_json(path, orient='records', indent=2)\n",
    "            logger.info(f\"Exportado: {path}\")\n",
    "        \n",
    "        if 'excel' in formats:\n",
    "            try:\n",
    "                path = f\"{base_path}_clean.xlsx\"\n",
    "                self.df_clean.to_excel(path, index=False)\n",
    "                logger.info(f\"Exportado: {path}\")\n",
    "            except ImportError:\n",
    "                logger.warning(\"openpyxl no instalado, omitiendo Excel\")\n",
    "    \n",
    "    def run(self, cleaning_func: Optional[Callable] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Ejecuta pipeline completo.\n",
    "        \n",
    "        Returns:\n",
    "            M√©tricas del an√°lisis\n",
    "        \"\"\"\n",
    "        logger.info(f\"Ejecutando pipeline '{self.name}'\")\n",
    "        \n",
    "        # 1. Limpiar\n",
    "        self.clean_data(cleaning_func)\n",
    "        \n",
    "        # 2. Analizar\n",
    "        metrics = self.analyze()\n",
    "        \n",
    "        # 3. Exportar\n",
    "        self.export(['csv', 'json'])\n",
    "        \n",
    "        logger.info(f\"Pipeline '{self.name}' completado\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Demostraci√≥n del pipeline\n",
    "print(\"=\"*70)\n",
    "print(\"DEMOSTRACI√ìN DEL PIPELINE COMPLETO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pipeline = ScrapingPipeline(name=\"wikipedia_empresas\", output_dir=\"./output_pipeline\")\n",
    "pipeline.raw_data = df_wikipedia.to_dict('records')\n",
    "metrics = pipeline.run(cleaning_func=limpiar_datos_scrapeados)\n",
    "\n",
    "print(f\"\\nüìä M√©tricas del an√°lisis:\")\n",
    "print(f\"   Total registros: {metrics['total_records']}\")\n",
    "print(f\"   Columnas: {metrics['columns']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
