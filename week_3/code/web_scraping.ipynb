{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2acf22b",
   "metadata": {},
   "source": [
    "### WEBSCRAPPING\n",
    "\n",
    "## Recommendations to run this notebook\n",
    "\n",
    "You might need to install some of the libraries being used, you can do so by CREATING A VIRTUAL ENVIRONMENT AND USE:\n",
    "\n",
    "\n",
    "```python\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e599d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#s=Service(ChromeDriverManager().install()) #MAC user might need this\n",
    "#driver = webdriver.Chrome(service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93e72822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# This will automatically download and use the correct ChromeDriver version\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a6077226",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('http://twitter.com/login')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up WebDriver...\n",
      "WebDriver setup complete.\n",
      "Entering email...\n",
      "Clicking 'Next'...\n",
      "Retrying to click 'Next'... Attempt 1\n",
      "Retrying to click 'Next'... Attempt 2\n",
      "Retrying to click 'Next'... Attempt 3\n",
      "Entering password...\n",
      "Login successful!\n"
     ]
    }
   ],
   "source": [
    "class TwitterScraper:\n",
    "    def __init__(self, email, password):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        self.driver = self._initialize_driver()\n",
    "\n",
    "    def _initialize_driver(self):\n",
    "        print(\"Setting up WebDriver...\")\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--log-level=3\")\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        \n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "            print(\"WebDriver setup complete.\")\n",
    "            return driver\n",
    "        except WebDriverException as e:\n",
    "            print(f\"Error setting up WebDriver: {e}\")\n",
    "            raise\n",
    "\n",
    "    def login(self):\n",
    "        try:\n",
    "            self.driver.get(TWITTER_LOGIN_URL)\n",
    "            time.sleep(5)\n",
    "\n",
    "            print(\"Entering email...\")\n",
    "            self._enter_email()\n",
    "            print(\"Clicking 'Next'...\")\n",
    "            self._click_next_button()\n",
    "            print(\"Entering password...\")\n",
    "            self._enter_password()\n",
    "\n",
    "            print(\"Login successful!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during login: {e}\")\n",
    "        finally:\n",
    "            time.sleep(5)\n",
    "            self.driver.quit()\n",
    "\n",
    "    def _enter_email(self):\n",
    "        try:\n",
    "            email_input = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete='username']\"))\n",
    "            )\n",
    "            email_input.send_keys(self.email)\n",
    "            email_input.send_keys(Keys.RETURN)\n",
    "        except NoSuchElementException:\n",
    "            print(\"Email input field not found.\")\n",
    "\n",
    "    def _click_next_button(self):\n",
    "        attempts = 0\n",
    "        while attempts < 3:\n",
    "            try:\n",
    "                next_button = WebDriverWait(self.driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//div[@role='button' and text()='Next']\"))\n",
    "                )\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                next_button.click()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Retrying to click 'Next'... Attempt {attempts + 1}\")\n",
    "                attempts += 1\n",
    "                time.sleep(2)\n",
    "\n",
    "    def _enter_password(self):\n",
    "        try:\n",
    "            password_input = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//input[@autocomplete='current-password']\"))\n",
    "            )\n",
    "            password_input.send_keys(self.password)\n",
    "            password_input.send_keys(Keys.RETURN)\n",
    "        except NoSuchElementException:\n",
    "            print(\"Password input field not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error entering password: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    email = \"jose.perez.castellanos@itam.mx\"\n",
    "    password = \"...\"\n",
    "\n",
    "    scraper = TwitterScraper(email=email, password=password)\n",
    "    scraper.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f7425dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Next page button not found. Exiting.\n",
      "Total run time: 10.602749109268188 seconds\n",
      "Quotes have been scraped and saved to quotes.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Recording the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initializing Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Opening the website\n",
    "driver.get(\"http://quotes.toscrape.com/js/\")\n",
    "\n",
    "# Initializing lists to store data\n",
    "quotes_data = []\n",
    "\n",
    "# Creating a WebDriverWait object with a timeout of 10 seconds\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Iterating through all pages\n",
    "while True:\n",
    "    # Finding all the quotes on the current page\n",
    "    quotes = driver.find_elements(By.CLASS_NAME, \"quote\")\n",
    "    \n",
    "    # Extracting data for each quote on the current page\n",
    "    for quote in quotes:\n",
    "        quote_text = quote.find_element(By.CLASS_NAME, \"text\").text\n",
    "        author = quote.find_element(By.CLASS_NAME, \"author\").text\n",
    "        tags = \"|\".join(tag.text for tag in quote.find_elements(By.CLASS_NAME, \"tag\"))\n",
    "        \n",
    "        # Appending data to the list\n",
    "        quotes_data.append([author, quote_text, tags])\n",
    "    \n",
    "    # Checking if there is a next page\n",
    "    try:\n",
    "        next_page = driver.find_element(By.XPATH, \"//li[@class='next']/a\")\n",
    "        if 'disabled' in next_page.get_attribute(\"class\"):\n",
    "            print(\"Reached the last page.\")\n",
    "            break\n",
    "    except:\n",
    "        print(\"Next page button not found. Exiting.\")\n",
    "        break\n",
    "    \n",
    "    # Scrolling to the next page element\n",
    "    print(\"Scrolling to the next page.\")\n",
    "    ActionChains(driver).move_to_element(next_page).perform()\n",
    "    \n",
    "    # Clicking the \"Next\" link\n",
    "    print(\"Clicking the next page.\")\n",
    "    next_page.click()\n",
    "\n",
    "# Closing the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Recording the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculating the total run time\n",
    "total_run_time = end_time - start_time\n",
    "print(f\"Total run time: {total_run_time} seconds\")\n",
    "\n",
    "# Saving the data to a CSV file\n",
    "with open(\"quotes.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # Writing header row\n",
    "    csv_writer.writerow([\"Author\", \"Quote\", \"Tags\"])\n",
    "    \n",
    "    # Writing quote data\n",
    "    csv_writer.writerows(quotes_data)\n",
    "\n",
    "print(\"Quotes have been scraped and saved to quotes.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dee0d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Author                                              Quote  \\\n",
      "0  Albert Einstein  “The world as we have created it is a process ...   \n",
      "1     J.K. Rowling  “It is our choices, Harry, that show what we t...   \n",
      "2  Albert Einstein  “There are only two ways to live your life. On...   \n",
      "3      Jane Austen  “The person, be it gentleman or lady, who has ...   \n",
      "4   Marilyn Monroe  “Imperfection is beauty, madness is genius and...   \n",
      "\n",
      "                                       Tags  \n",
      "0       change|deep-thoughts|thinking|world  \n",
      "1                         abilities|choices  \n",
      "2  inspirational|life|live|miracle|miracles  \n",
      "3             aliteracy|books|classic|humor  \n",
      "4                 be-yourself|inspirational  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"quotes.csv\")\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81dc0128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert Einstein', 'j.K. Rowling', 'Mark Taiwn', 'C.bS. Lweis']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def introduce_errors(name, num_errors=1):\n",
    "    \"\"\"Introduce errors in a name by adding, removing, or swapping letters.\"\"\"\n",
    "    name = list(name)\n",
    "    for _ in range(num_errors):\n",
    "        error_type = random.choice([\"add\", \"remove\", \"swap\"])\n",
    "        if error_type == \"add\" and len(name) > 0:\n",
    "            index = random.randint(0, len(name) - 1)\n",
    "            name.insert(index, random.choice(\"abcdefghijklmnopqrstuvwxyz\"))\n",
    "        elif error_type == \"remove\" and len(name) > 1:\n",
    "            index = random.randint(0, len(name) - 1)\n",
    "            name.pop(index)\n",
    "        elif error_type == \"swap\" and len(name) > 1:\n",
    "            index = random.randint(0, len(name) - 2)\n",
    "            name[index], name[index + 1] = name[index + 1], name[index]\n",
    "    return ''.join(name)\n",
    "\n",
    "# Example usage:\n",
    "authors = [\"Albert Einstein\", \"J.K. Rowling\", \"Mark Twain\", \"C.S. Lewis\"]\n",
    "authors_with_errors = [introduce_errors(name, num_errors=2) for name in authors]\n",
    "print(authors_with_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04e7fdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 'Albert Einstein' and 'Alberton Einstain': 3\n"
     ]
    }
   ],
   "source": [
    "def manual_levenshtein_distance(name1, name2):\n",
    "    \"\"\"Compute the Levenshtein distance between two strings manually.\"\"\"\n",
    "    len1, len2 = len(name1), len(name2)\n",
    "    dp = [[0 for _ in range(len2 + 1)] for _ in range(len1 + 1)]\n",
    "\n",
    "    # Initialize the DP table\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Fill the table\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            cost = 0 if name1[i - 1] == name2[j - 1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,  # Deletion\n",
    "                dp[i][j - 1] + 1,  # Insertion\n",
    "                dp[i - 1][j - 1] + cost,  # Substitution\n",
    "            )\n",
    "\n",
    "    return dp[len1][len2]\n",
    "\n",
    "# Example usage\n",
    "correct_name = \"Albert Einstein\"\n",
    "misspelled_name = \"Alberton Einstain\"\n",
    "distance = manual_levenshtein_distance(correct_name, misspelled_name)\n",
    "print(f\"Distance between '{correct_name}' and '{misspelled_name}': {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7b9afaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Scrolling to the next page.\n",
      "Clicking the next page.\n",
      "Next page button not found. Exiting.\n",
      "Total run time: 10.36624002456665 seconds\n",
      "Quotes have been scraped and saved to quotes_filtered.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Manual Levenshtein distance function\n",
    "def levenshtein_distance(name1, name2):\n",
    "    \"\"\"Manually calculate the Levenshtein distance between two strings.\"\"\"\n",
    "    len1, len2 = len(name1), len(name2)\n",
    "    dp = [[0 for _ in range(len2 + 1)] for _ in range(len1 + 1)]\n",
    "\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            cost = 0 if name1[i - 1] == name2[j - 1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,    # Deletion\n",
    "                dp[i][j - 1] + 1,    # Insertion\n",
    "                dp[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "    return dp[len1][len2]\n",
    "\n",
    "# Function to check if a name is similar within a threshold\n",
    "def is_similar_name(correct_name, author_name, max_distance=3):\n",
    "    return levenshtein_distance(correct_name, author_name) <= max_distance\n",
    "\n",
    "# Recording the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initializing Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Opening the website\n",
    "driver.get(\"http://quotes.toscrape.com/js/\")\n",
    "\n",
    "# Initializing lists to store data\n",
    "quotes_data = []\n",
    "\n",
    "# Creating a WebDriverWait object with a timeout of 10 seconds\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Correct name and threshold for similarity\n",
    "correct_name = \"Albert Einstein\"\n",
    "max_distance = 3\n",
    "\n",
    "# Iterating through all pages\n",
    "while True:\n",
    "    # Finding all the quotes on the current page\n",
    "    quotes = driver.find_elements(By.CLASS_NAME, \"quote\")\n",
    "    \n",
    "    # Extracting data for each quote on the current page\n",
    "    for quote in quotes:\n",
    "        try:\n",
    "            quote_text = quote.find_element(By.CLASS_NAME, \"text\").text\n",
    "            author = quote.find_element(By.CLASS_NAME, \"author\").text\n",
    "            tags = \"|\".join(tag.text for tag in quote.find_elements(By.CLASS_NAME, \"tag\"))\n",
    "            \n",
    "            # Check if the author's name is similar to the target name\n",
    "            if is_similar_name(correct_name, author, max_distance):\n",
    "                quotes_data.append([author, quote_text, tags])\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting quote: {e}\")\n",
    "\n",
    "    # Checking if there is a next page\n",
    "    try:\n",
    "        next_page = driver.find_element(By.XPATH, \"//li[@class='next']/a\")\n",
    "    except:\n",
    "        print(\"Next page button not found. Exiting.\")\n",
    "        break\n",
    "\n",
    "    # Scrolling to the next page element\n",
    "    print(\"Scrolling to the next page.\")\n",
    "    ActionChains(driver).move_to_element(next_page).perform()\n",
    "    \n",
    "    # Clicking the \"Next\" link\n",
    "    print(\"Clicking the next page.\")\n",
    "    next_page.click()\n",
    "\n",
    "# Closing the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Recording the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculating the total run time\n",
    "total_run_time = end_time - start_time\n",
    "print(f\"Total run time: {total_run_time} seconds\")\n",
    "\n",
    "# Saving the data to a CSV file\n",
    "with open(\"quotes_filtered.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # Writing header row\n",
    "    csv_writer.writerow([\"Author\", \"Quote\", \"Tags\"])\n",
    "    \n",
    "    # Writing quote data\n",
    "    csv_writer.writerows(quotes_data)\n",
    "\n",
    "print(\"Quotes have been scraped and saved to quotes_filtered.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b676a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Author                                              Quote  \\\n",
      "0  Albert Einstein  “The world as we have created it is a process ...   \n",
      "1  Albert Einstein  “There are only two ways to live your life. On...   \n",
      "2  Albert Einstein  “Try not to become a man of success. Rather be...   \n",
      "3  Albert Einstein  “If you can't explain it to a six year old, yo...   \n",
      "4  Albert Einstein  “If you want your children to be intelligent, ...   \n",
      "\n",
      "                                       Tags  \n",
      "0       change|deep-thoughts|thinking|world  \n",
      "1  inspirational|life|live|miracle|miracles  \n",
      "2                   adulthood|success|value  \n",
      "3                     simplicity|understand  \n",
      "4                      children|fairy-tales  \n"
     ]
    }
   ],
   "source": [
    "# Load the filtered data\n",
    "df = pd.read_csv(\"quotes_filtered.csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea4ac4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 articles for 'D. Trump'\n",
      "Found 35 articles for 'Donald T.'\n",
      "Found 96 articles for 'Donald Trump'\n",
      "\n",
      "Filtered news saved to 'filtered_news_last_hour.csv'.\n",
      "\n",
      "Extracted Links:\n",
      "   Name Variation                                               Link\n",
      "0        D. Trump  https://news.google.com/read/CBMihgFBVV95cUxQM...\n",
      "1        D. Trump  https://news.google.com/read/CBMiuAFBVV95cUxOb...\n",
      "2        D. Trump  https://news.google.com/read/CBMiuwFBVV95cUxNR...\n",
      "3        D. Trump  https://news.google.com/read/CBMikwJBVV95cUxQW...\n",
      "4        D. Trump  https://news.google.com/read/CBMikwFBVV95cUxQV...\n",
      "5       Donald T.  https://news.google.com/read/CBMiX0FVX3lxTE9MM...\n",
      "6       Donald T.  https://news.google.com/read/CBMirAFBVV95cUxOM...\n",
      "7       Donald T.  https://news.google.com/read/CBMikwJBVV95cUxQW...\n",
      "8       Donald T.  https://news.google.com/read/CBMipwFBVV95cUxQU...\n",
      "9       Donald T.  https://news.google.com/read/CBMiZ0FVX3lxTE1TZ...\n",
      "10   Donald Trump  https://news.google.com/read/CBMirAFBVV95cUxOM...\n",
      "11   Donald Trump  https://news.google.com/read/CBMihgFBVV95cUxQM...\n",
      "12   Donald Trump  https://news.google.com/read/CBMikwJBVV95cUxQW...\n",
      "13   Donald Trump  https://news.google.com/read/CBMikwFBVV95cUxPV...\n",
      "14   Donald Trump  https://news.google.com/read/CBMiX0FVX3lxTE9MM...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "try:\n",
    "    # List of name variations to search\n",
    "    name_variations = [\"D. Trump\", \"Donald T.\", \"Donald Trump\"]  \n",
    "\n",
    "    # Initialize a dictionary to store results\n",
    "    news_data = {name: [] for name in name_variations}\n",
    "\n",
    "    # Iterate through each name variation\n",
    "    for name in name_variations:\n",
    "        # Construct the search URL with the name variation and last hour filter\n",
    "        search_query = f\"{name} when:1h\"\n",
    "        url = f\"https://news.google.com/search?q={search_query.replace(' ', '%20')}&hl=en-US&gl=US&ceid=US%3Aen\"\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "        # Locate all articles\n",
    "        articles = driver.find_elements(By.XPATH, \"//article\")\n",
    "        print(f\"Found {len(articles)} articles for '{name}'\")\n",
    "\n",
    "        # Extract the first 5 links\n",
    "        for article in articles[:5]:\n",
    "            try:\n",
    "                # Extract the link\n",
    "                link_element = article.find_element(By.XPATH, \".//a[@href]\")\n",
    "                link = link_element.get_attribute(\"href\")\n",
    "\n",
    "                # Append the link to the corresponding name variation\n",
    "                news_data[name].append(link)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article for '{name}': {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "# Save results to a DataFrame\n",
    "results = []\n",
    "for name, links in news_data.items():\n",
    "    for link in links:\n",
    "        results.append({\"Name Variation\": name, \"Link\": link})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"filtered_news_last_hour.csv\", index=False)\n",
    "print(\"\\nFiltered news saved to 'filtered_news_last_hour.csv'.\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nExtracted Links:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "68d37cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/15: Scraped - Title: Trump’s Executive Orders: Reversing Biden’s Policies and Attacking the ‘Deep State’\n",
      "2/15: Scraped - Title: Trump said he'd quickly end Russia's war on Ukraine. But it's proving tough.\n",
      "3/15: Scraped - Title: Trump to visit disaster zones in North Carolina and California on first trip of second term\n",
      "4/15: Scraped - Title: Donald Trump news LIVE: Post US exit, ordered by Trump, WHO to cut costs, ‘reset’ priorities, report says\n",
      "5/15: Scraped - Title: Trump vows ‘big discussion’ on FEMA ahead of disaster tour\n",
      "6/15: Scraped - Title: Project 1897: The imperial presidency\n",
      "7/15: Scraped - Title: How will Trump try to fix inflation? Experts say he'll focus on these areas.\n",
      "8/15: Scraped - Title: Donald Trump news LIVE: Post US exit, ordered by Trump, WHO to cut costs, ‘reset’ priorities, report says\n",
      "9/15: Scraped - Title: Trump won’t deliver on maximum tariff pledges, says his former commerce secretary—merely making the threat is enough\n",
      "10/15: Scraped - Title: This site can’t be reached\n",
      "11/15: Scraped - Title: How will Trump try to fix inflation? Experts say he'll focus on these areas.\n",
      "12/15: Scraped - Title: Trump’s Executive Orders: Reversing Biden’s Policies and Attacking the ‘Deep State’\n",
      "13/15: Scraped - Title: Donald Trump news LIVE: Post US exit, ordered by Trump, WHO to cut costs, ‘reset’ priorities, report says\n",
      "14/15: Scraped - Title: This site can’t be reached\n",
      "15/15: Scraped - Title: Project 1897: The imperial presidency\n",
      "WebDriver closed.\n",
      "Scraped data saved to 'scraped_articles_titles_only.csv'.\n",
      "\n",
      "Scraped Data Preview:\n",
      "                                                Link  \\\n",
      "0  https://news.google.com/read/CBMihgFBVV95cUxQM...   \n",
      "1  https://news.google.com/read/CBMiuAFBVV95cUxOb...   \n",
      "2  https://news.google.com/read/CBMiuwFBVV95cUxNR...   \n",
      "3  https://news.google.com/read/CBMikwJBVV95cUxQW...   \n",
      "4  https://news.google.com/read/CBMikwFBVV95cUxQV...   \n",
      "\n",
      "                                               Title  \n",
      "0  Trump’s Executive Orders: Reversing Biden’s Po...  \n",
      "1  Trump said he'd quickly end Russia's war on Uk...  \n",
      "2  Trump to visit disaster zones in North Carolin...  \n",
      "3  Donald Trump news LIVE: Post US exit, ordered ...  \n",
      "4  Trump vows ‘big discussion’ on FEMA ahead of d...  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Load the CSV file with Google News links\n",
    "df = pd.read_csv(\"filtered_news_last_hour.csv\")\n",
    "article_links = df[\"Link\"].tolist()\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run browser in headless mode\n",
    "options.add_argument(\"--disable-gpu\")  # Disable GPU rendering for better performance\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "prefs = {\"profile.managed_default_content_settings.images\": 2}  # Disable image loading to speed up scraping\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Initialize a list to store scraped data\n",
    "scraped_data = []\n",
    "\n",
    "try:\n",
    "    for idx, link in enumerate(article_links, start=1):\n",
    "        try:\n",
    "            # Open the article link\n",
    "            driver.get(link)\n",
    "            driver.set_page_load_timeout(30)  # Allow up to 30 seconds for the page to load\n",
    "\n",
    "            # Wait for the article's main content to load\n",
    "            try:\n",
    "                WebDriverWait(driver, 15).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//h1\"))\n",
    "                )\n",
    "            except Exception:\n",
    "                print(f\"No <h1> tag found for link {idx}/{len(article_links)}.\")\n",
    "\n",
    "            # Scrape the article title\n",
    "            try:\n",
    "                # Primary: Look for <h1> tag\n",
    "                title = driver.find_element(By.XPATH, \"//h1\").text\n",
    "            except Exception:\n",
    "                try:\n",
    "                    # Fallback: Use the <title> tag\n",
    "                    title = driver.find_element(By.TAG_NAME, \"title\").text\n",
    "                except Exception:\n",
    "                    title = \"N/A\"  # Default value if no title is found\n",
    "\n",
    "            # Append the scraped data\n",
    "            scraped_data.append({\"Link\": link, \"Title\": title})\n",
    "            print(f\"{idx}/{len(article_links)}: Scraped - Title: {title}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing link {idx}/{len(article_links)}: {link}, Error: {e}\")\n",
    "        \n",
    "finally:\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    print(\"WebDriver closed.\")\n",
    "\n",
    "# Save the scraped data to a new CSV file\n",
    "output_file = \"scraped_articles_titles_only.csv\"\n",
    "scraped_df = pd.DataFrame(scraped_data)\n",
    "scraped_df.to_csv(output_file, index=False)\n",
    "print(f\"Scraped data saved to '{output_file}'.\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nScraped Data Preview:\")\n",
    "print(scraped_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraping_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
